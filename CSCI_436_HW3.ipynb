{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6458fb7a8cf0418690dfc81e02109e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1219d113754349468374f2cde9df955d",
              "IPY_MODEL_a4b39cad98634ee397400b6710540bcf"
            ],
            "layout": "IPY_MODEL_cd3327f731394930bd5187948986a4f7"
          }
        },
        "1219d113754349468374f2cde9df955d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56fbdfc0c724e0ab76d1636c3cb443e",
            "placeholder": "​",
            "style": "IPY_MODEL_dc10921233c84688ae12c9fc41f61fea",
            "value": "0.133 MB of 0.133 MB uploaded\r"
          }
        },
        "a4b39cad98634ee397400b6710540bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01897f2fcfd949ae940351af89a8544e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_440aa0237a574426bbb4c94dc65ebb97",
            "value": 1
          }
        },
        "cd3327f731394930bd5187948986a4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56fbdfc0c724e0ab76d1636c3cb443e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc10921233c84688ae12c9fc41f61fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01897f2fcfd949ae940351af89a8544e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440aa0237a574426bbb4c94dc65ebb97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stEisanTxIbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47666963-dab3-412e-c0ca-3a0d0ecba0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "hmP68NUHry0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44d0036-8d29-40d5-f669-aadc4075f16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Import packages'''\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import wandb ##weight and bias\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "84uSeSfTrnMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Part 1 - Loads in data from DATA_PATH variable using se the data loader about MNIST from Pytorch libs\n",
        "def _load_data(DATA_PATH, batch_size):\n",
        "    ## for training\n",
        "    rotation = 15\n",
        "    train_trans = transforms.Compose([transforms.RandomRotation(rotation),\\\n",
        "                                      transforms.RandomHorizontalFlip(),\\\n",
        "                                      transforms.ToTensor(),\\\n",
        "                                      transforms.Normalize((0.5), (0.5))])\n",
        "    train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, download=True,\\\n",
        "                                               train=True, transform=train_trans)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\\\n",
        "                              shuffle=True, num_workers=0)\n",
        "    ## for testing\n",
        "    test_trans = transforms.Compose([transforms.ToTensor(),\\\n",
        "                                     transforms.Normalize((0.5), (0.5))])\n",
        "    test_dataset = torchvision.datasets.MNIST(root=DATA_PATH,\\\n",
        "                                              download=True, train=False, transform=test_trans)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\\\n",
        "                             shuffle=False, num_workers=0)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "laAZoXS0r4A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - CNN model class definition, defined model architecture\n",
        "class MLPModel(nn.Module):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self,):\n",
        "        super(MLPModel, self).__init__()\n",
        "        ##-----------------------------------------------------------\n",
        "        ## define the model architecture here\n",
        "        ## MNIST image input size batch * 28 * 28 (one input channel)\n",
        "        ##-----------------------------------------------------------\n",
        "\n",
        "        ## Write code about three MLP layers below\n",
        "        self.mlp = nn.Sequential(nn.Linear(28*28,100),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.2),\n",
        "                                nn.Linear(100,50),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(50,10)\n",
        "                                )\n",
        "    '''feed features to the model'''\n",
        "    def forward(self, x):\n",
        "        ## write flatten tensor code below [I have done it]\n",
        "        x = torch.flatten(x,1)\n",
        "        ## ---------------------------------------------------\n",
        "        ## write code about MLP predict results\n",
        "        ## ---------------------------------------------------\n",
        "        result = self.mlp(x)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "o-qshIErr9nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the 3 layers and 2 layers\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPModel, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1)  # 4 output channels\n",
        "        # BONUS - add batch normalization\n",
        "        self.bn1 = nn.BatchNorm2d(4)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # BONUS - max pooling to reduce dimensions\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout after the first convolutional layer\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        # 8 output channels\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
        "        # BONUS - add batch normalization\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.relu2 = nn.ReLU()\n",
        "         # BONUS - max pooling to reduce dimensions\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "         # Dropout after the second convolutional layer\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        # Calculate the number of features after convolutional and pooling operations\n",
        "        self.num_features = 8 * 7 * 7  # Adjusted based on the last pooling layer output size\n",
        "\n",
        "        # Fully connected layers\n",
        "\n",
        "        # Reduced number of units in the fully connected layer\n",
        "        self.fc1 = nn.Linear(self.num_features, 32)\n",
        "        # BONUS - add batch normalization\n",
        "        self.bn_fc1 = nn.BatchNorm1d(32)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        # Dropout after the first fully connected layer\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(32, 10)  # Output layer with 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, 1, 28, 28)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Apply dropout after the first convolutional layer\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Apply dropout after the second convolutional layer\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Flatten the output of conv layers and reshape to (batch_size, num_features)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # Apply dropout after the first fully connected layer\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "e3a7WCRykYPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## compute accuracy of training and testing\n",
        "def _compute_counts(y_pred, y_batch, mode='train'):\n",
        "    return (y_pred==y_batch).sum().item()"
      ],
      "metadata": {
        "id": "PQjNPGTyr_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(learning_rate, optimizer, epoch, decay):\n",
        "    \"\"\"initial LR decayed by 1/10 every args.lr epochs\"\"\"\n",
        "    lr = learning_rate\n",
        "    if (epoch > 5):\n",
        "        lr = 0.001\n",
        "    if (epoch >= 10):\n",
        "        lr = 0.0001\n",
        "    if (epoch > 20):\n",
        "        lr = 0.00001\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "14E5-lcQsBdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_checkpoint(ckp_path, model, epoch, optimizer, global_step):\n",
        "    ## save checkpoint to ckp_path: 'checkpoint/step_100.pt'\n",
        "    ckp_path = ckp_path + 'ckp_{}.pt'.format(epoch+1)\n",
        "    checkpoint = {'epoch': epoch,\n",
        "                  'global_step': global_step,\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict()}\n",
        "    torch.save(checkpoint, ckp_path)"
      ],
      "metadata": {
        "id": "xwwTYlwzsEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9qdnS3X6sl2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d5f456-b26d-4b42-cc31-14489e9a1f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set up and parameters for model\n",
        "def main():\n",
        "    ## choose cpu or gpu\n",
        "    seed = 1\n",
        "    torch.manual_seed(seed)\n",
        "    ## numpy.rand(1), 1.1\n",
        "    ## choose GPU id\n",
        "    gpu_id = 0  ## 1, 2, 3,4\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    if use_cuda:\n",
        "        device = torch.device('cuda', gpu_id)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    print(\"device: \", device)\n",
        "    ## random seed for cuda\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(72)\n",
        "\n",
        "    ## initialize hyper-parameters\n",
        "    num_epochs = 10\n",
        "    decay = 0.01\n",
        "    learning_rate = 0.0001\n",
        "    batch_size = 50 #100\n",
        "    ckp_path = './checkpoint/'\n",
        "    os.makedirs(ckp_path, exist_ok=True)\n",
        "\n",
        "    ## Part 1: loading in data using MNIST (see _load_data function definition above)\n",
        "    DATA_PATH = '.drive/shareddrives/hw3_csci436/data/'\n",
        "    train_loader, test_loader=_load_data(DATA_PATH, batch_size)\n",
        "\n",
        "    ## Part 2: load the MLP model in Model class definition above, this has the architecture for the CNN model\n",
        "    model =  MLPModel()\n",
        "    ## load model to gpu or cpu\n",
        "    model.to(device)\n",
        "\n",
        "    ## Part 3: define the Opimization method and LOSS FUNCTION: cross-entropy\n",
        "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)  ## optimizer\n",
        "    loss_fun = nn.CrossEntropyLoss()    ## cross entropy loss\n",
        "\n",
        "    ## load checkpoint below\n",
        "\n",
        "    # Begin model training\n",
        "    iteration = 0\n",
        "    model = model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        # Adjust learning rate\n",
        "        adjust_learning_rate(learning_rate, optimizer, epoch, decay)\n",
        "        \n",
        "        for batch_id, (x_batch, y_labels) in enumerate(train_loader):\n",
        "            iteration += 1\n",
        "            x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output_y = model(x_batch)\n",
        "            loss = loss_fun(output_y, y_labels)\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy\n",
        "            y_pred = torch.argmax(output_y, 1)\n",
        "            accy = _compute_counts(y_pred, y_labels) / batch_size\n",
        "\n",
        "            # Print and log loss and accuracy\n",
        "            if iteration % 10 == 0:\n",
        "                print('iter: {} loss: {:.4f}, accy: {:.4f}'.format(iteration, loss.item(), accy))\n",
        "\n",
        "        # Save checkpoint at the end of each epoch\n",
        "        _save_checkpoint(ckp_path, model, epoch, optimizer, iteration)\n",
        "\n",
        "        # Stop training if the current epoch is the last epoch\n",
        "        if epoch + 1 == num_epochs:\n",
        "            break"
      ],
      "metadata": {
        "id": "iTgWDi11dlRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up and parameters for model\n",
        "def main():\n",
        "    ## choose cpu or gpu\n",
        "    seed = 1\n",
        "    torch.manual_seed(seed)\n",
        "    ## numpy.rand(1), 1.1\n",
        "    ## choose GPU id\n",
        "    gpu_id = 0  ## 1, 2, 3,4\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    if use_cuda:\n",
        "        device = torch.device('cuda', gpu_id)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    print(\"device: \", device)\n",
        "    ## random seed for cuda\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(72)\n",
        "\n",
        "    ## initialize hyper-parameters\n",
        "    num_epochs = 10\n",
        "    decay = 0.01\n",
        "    learning_rate = 0.0001\n",
        "    batch_size = 50 #100\n",
        "    ckp_path = './checkpoint/'\n",
        "    os.makedirs(ckp_path, exist_ok=True)\n",
        "\n",
        "    ## Part 1: loading in data using MNIST (see _load_data function definition above)\n",
        "    DATA_PATH = '.drive/shareddrives/hw3_csci436/data/'\n",
        "    # for graders: './data/'\n",
        "    train_loader, test_loader=_load_data(DATA_PATH, batch_size)\n",
        "\n",
        "    ## Part 2: load the MLP model in Model class definition above, this has the architecture for the CNN model\n",
        "    model =  MLPModel()\n",
        "    ## load model to gpu or cpu\n",
        "    model.to(device)\n",
        "\n",
        "    ## Part 3: define the Opimization method and LOSS FUNCTION: cross-entropy\n",
        "    ## optimizer\n",
        "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "    ## cross entropy loss\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Visualization - Lists to store logged data\n",
        "    iterations = []\n",
        "    losses = []\n",
        "\n",
        "    ## load checkpoint below\n",
        "\n",
        "    ##  model training\n",
        "    iteration = 0\n",
        "    if True:\n",
        "        model = model.train() ## model training\n",
        "        for epoch in range(num_epochs): #10-50\n",
        "            ## learning rate\n",
        "            adjust_learning_rate(learning_rate, optimizer, epoch, decay)\n",
        "            for batch_id, (x_batch,y_labels) in enumerate(train_loader):\n",
        "\n",
        "                iteration += 1\n",
        "                x_batch,y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
        "\n",
        "                output_y = model(x_batch)\n",
        "                ##--------------------------------------------------------------\n",
        "                ## Step 4: compute loss between ground truth and predicted result\n",
        "                ##---------------------------------------------------------------\n",
        "                loss = loss_fun(output_y, y_labels)\n",
        "\n",
        "                ##----------------------------------------------\n",
        "                ## Step 5: write back propagation steps below\n",
        "                ##----------------------------------------------\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step() # update params\n",
        "\n",
        "                ##---------------------------------------------------------\n",
        "                ## Step 6: get the predict result and then compute accuracy\n",
        "                ##---------------------------------------------------------\n",
        "                y_pred = torch.argmax(output_y.data, 1)\n",
        "                accy = _compute_counts(y_pred, y_labels)/batch_size\n",
        "                ##----------------------------------------------------------\n",
        "                ## Step 7: print loss values [I have done it]\n",
        "                ##----------------------------------------------------------\n",
        "                if iteration%10==0:\n",
        "                    print('iter: {} loss: {}, accy: {}'.format(iteration, loss.item(), accy))\n",
        "                    wandb.log({'iter': iteration, 'loss': loss.item()})\n",
        "                    wandb.log({'iter': iteration, 'accy': accy})\n",
        "\n",
        "                    # Append data for visualization\n",
        "                    iterations.append(iteration)\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "            ##---------------------------------------------------\n",
        "            ##    save checkpoint below\n",
        "            ##---------------------------------------------------\n",
        "            _save_checkpoint(ckp_path, model, epoch, optimizer, iteration)\n",
        "\n",
        "    ##------------------------------------\n",
        "    ##    model testing code below\n",
        "    ##------------------------------------\n",
        "    total = 0\n",
        "    accy_count = 0\n",
        "    model.eval() ##test\n",
        "    with torch.no_grad(): ## no gradient update\n",
        "        for batch_id, (x_batch,y_labels) in enumerate(test_loader):\n",
        "            x_batch, y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
        "            ##---------------------------------------\n",
        "            ## Step 8: write the predict result below\n",
        "            ##---------------------------------------\n",
        "            output_y = model(x_batch)\n",
        "            y_pred = torch.argmax(output_y.data, 1)\n",
        "\n",
        "            ##--------------------------------------------------\n",
        "            ## Step 9: computing the test accuracy\n",
        "            ##---------------------------------------------------\n",
        "            total += len(y_labels)\n",
        "            accy_count += _compute_counts(y_pred, y_labels)\n",
        "    accy = accy_count/total\n",
        "    print(\"testing accy: \", accy)\n",
        "    wandb.log({'Testing Accuracy': accy})\n",
        "\n",
        "    # Log training loss data as a table\n",
        "    table = wandb.Table(data=list(zip(iterations, losses)), columns=[\"Iteration\", \"Loss\"])\n",
        "    wandb.log({\"Training Loss Table\": table})\n",
        "\n",
        "    # Visualize training loss with wandb\n",
        "    wandb.log({\"Training Loss Plot\": wandb.plot.line(table, x='Iteration', y='Loss', title='Training Loss vs Iteration')})"
      ],
      "metadata": {
        "id": "hRecvNskedc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(project='CSCI_436', name='hw3',settings=wandb.Settings(start_method=\"fork\")):\n",
        "    main()\n",
        "    ## %wandb cmhwang/CSCI_436/runs/hw3"
      ],
      "metadata": {
        "id": "z7XLOz3pv-A9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6458fb7a8cf0418690dfc81e02109e19",
            "1219d113754349468374f2cde9df955d",
            "a4b39cad98634ee397400b6710540bcf",
            "cd3327f731394930bd5187948986a4f7",
            "f56fbdfc0c724e0ab76d1636c3cb443e",
            "dc10921233c84688ae12c9fc41f61fea",
            "01897f2fcfd949ae940351af89a8544e",
            "440aa0237a574426bbb4c94dc65ebb97"
          ]
        },
        "outputId": "8d2ece7b-ce13-497b-edf8-d53d68318a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240418_194859-db15qdjh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hw6_csci436/CSCI_436/runs/db15qdjh' target=\"_blank\">hw3</a></strong> to <a href='https://wandb.ai/hw6_csci436/CSCI_436' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hw6_csci436/CSCI_436' target=\"_blank\">https://wandb.ai/hw6_csci436/CSCI_436</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hw6_csci436/CSCI_436/runs/db15qdjh' target=\"_blank\">https://wandb.ai/hw6_csci436/CSCI_436/runs/db15qdjh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cpu\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 133072510.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .drive/shareddrives/hw3_csci436/data/MNIST/raw/train-images-idx3-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20745965.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .drive/shareddrives/hw3_csci436/data/MNIST/raw/train-labels-idx1-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46620948.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .drive/shareddrives/hw3_csci436/data/MNIST/raw/t10k-images-idx3-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 798596.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .drive/shareddrives/hw3_csci436/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to .drive/shareddrives/hw3_csci436/data/MNIST/raw\n",
            "\n",
            "iter: 10 loss: 2.52474045753479, accy: 0.08\n",
            "iter: 20 loss: 2.3848912715911865, accy: 0.12\n",
            "iter: 30 loss: 2.363461971282959, accy: 0.2\n",
            "iter: 40 loss: 2.197234869003296, accy: 0.14\n",
            "iter: 50 loss: 2.2286996841430664, accy: 0.18\n",
            "iter: 60 loss: 2.0984482765197754, accy: 0.28\n",
            "iter: 70 loss: 2.215816020965576, accy: 0.14\n",
            "iter: 80 loss: 2.2011024951934814, accy: 0.16\n",
            "iter: 90 loss: 2.228703498840332, accy: 0.22\n",
            "iter: 100 loss: 2.2909884452819824, accy: 0.1\n",
            "iter: 110 loss: 2.146059036254883, accy: 0.24\n",
            "iter: 120 loss: 2.1666970252990723, accy: 0.24\n",
            "iter: 130 loss: 2.1858251094818115, accy: 0.18\n",
            "iter: 140 loss: 2.170069932937622, accy: 0.16\n",
            "iter: 150 loss: 2.2748501300811768, accy: 0.14\n",
            "iter: 160 loss: 2.009744167327881, accy: 0.24\n",
            "iter: 170 loss: 2.114798069000244, accy: 0.22\n",
            "iter: 180 loss: 2.117863416671753, accy: 0.22\n",
            "iter: 190 loss: 2.0061817169189453, accy: 0.22\n",
            "iter: 200 loss: 2.0767362117767334, accy: 0.26\n",
            "iter: 210 loss: 2.0642011165618896, accy: 0.34\n",
            "iter: 220 loss: 2.094486951828003, accy: 0.18\n",
            "iter: 230 loss: 2.0306944847106934, accy: 0.28\n",
            "iter: 240 loss: 2.019282817840576, accy: 0.28\n",
            "iter: 250 loss: 2.0914297103881836, accy: 0.16\n",
            "iter: 260 loss: 1.9050626754760742, accy: 0.36\n",
            "iter: 270 loss: 1.9246069192886353, accy: 0.38\n",
            "iter: 280 loss: 1.8708686828613281, accy: 0.4\n",
            "iter: 290 loss: 2.165621757507324, accy: 0.16\n",
            "iter: 300 loss: 2.017306089401245, accy: 0.3\n",
            "iter: 310 loss: 2.0414350032806396, accy: 0.26\n",
            "iter: 320 loss: 1.8972408771514893, accy: 0.26\n",
            "iter: 330 loss: 1.9267261028289795, accy: 0.4\n",
            "iter: 340 loss: 1.8909152746200562, accy: 0.42\n",
            "iter: 350 loss: 1.8592723608016968, accy: 0.46\n",
            "iter: 360 loss: 1.7979148626327515, accy: 0.36\n",
            "iter: 370 loss: 1.8898017406463623, accy: 0.34\n",
            "iter: 380 loss: 1.8091092109680176, accy: 0.38\n",
            "iter: 390 loss: 1.9446057081222534, accy: 0.3\n",
            "iter: 400 loss: 1.7926064729690552, accy: 0.5\n",
            "iter: 410 loss: 1.7973413467407227, accy: 0.38\n",
            "iter: 420 loss: 1.9434949159622192, accy: 0.32\n",
            "iter: 430 loss: 1.711195945739746, accy: 0.52\n",
            "iter: 440 loss: 1.8745455741882324, accy: 0.38\n",
            "iter: 450 loss: 1.82047700881958, accy: 0.4\n",
            "iter: 460 loss: 1.7875694036483765, accy: 0.4\n",
            "iter: 470 loss: 1.8115299940109253, accy: 0.44\n",
            "iter: 480 loss: 1.6610299348831177, accy: 0.48\n",
            "iter: 490 loss: 1.845353603363037, accy: 0.42\n",
            "iter: 500 loss: 1.7430601119995117, accy: 0.42\n",
            "iter: 510 loss: 1.6373170614242554, accy: 0.4\n",
            "iter: 520 loss: 1.8006978034973145, accy: 0.32\n",
            "iter: 530 loss: 1.8521406650543213, accy: 0.46\n",
            "iter: 540 loss: 1.7297886610031128, accy: 0.42\n",
            "iter: 550 loss: 1.7956185340881348, accy: 0.4\n",
            "iter: 560 loss: 1.6405025720596313, accy: 0.56\n",
            "iter: 570 loss: 1.637587308883667, accy: 0.42\n",
            "iter: 580 loss: 1.7172313928604126, accy: 0.4\n",
            "iter: 590 loss: 1.571295976638794, accy: 0.54\n",
            "iter: 600 loss: 1.7050461769104004, accy: 0.46\n",
            "iter: 610 loss: 1.6538113355636597, accy: 0.48\n",
            "iter: 620 loss: 1.6672883033752441, accy: 0.42\n",
            "iter: 630 loss: 1.6507149934768677, accy: 0.5\n",
            "iter: 640 loss: 1.6060738563537598, accy: 0.54\n",
            "iter: 650 loss: 1.6487784385681152, accy: 0.44\n",
            "iter: 660 loss: 1.696534276008606, accy: 0.42\n",
            "iter: 670 loss: 1.5930137634277344, accy: 0.48\n",
            "iter: 680 loss: 1.614619493484497, accy: 0.52\n",
            "iter: 690 loss: 1.5068750381469727, accy: 0.58\n",
            "iter: 700 loss: 1.8488322496414185, accy: 0.36\n",
            "iter: 710 loss: 1.4798469543457031, accy: 0.6\n",
            "iter: 720 loss: 1.6202435493469238, accy: 0.52\n",
            "iter: 730 loss: 1.6733745336532593, accy: 0.46\n",
            "iter: 740 loss: 1.6255676746368408, accy: 0.38\n",
            "iter: 750 loss: 1.5959149599075317, accy: 0.52\n",
            "iter: 760 loss: 1.7215049266815186, accy: 0.46\n",
            "iter: 770 loss: 1.6036022901535034, accy: 0.54\n",
            "iter: 780 loss: 1.729791283607483, accy: 0.4\n",
            "iter: 790 loss: 1.5461283922195435, accy: 0.54\n",
            "iter: 800 loss: 1.6256974935531616, accy: 0.52\n",
            "iter: 810 loss: 1.6066325902938843, accy: 0.48\n",
            "iter: 820 loss: 1.638502836227417, accy: 0.52\n",
            "iter: 830 loss: 1.5653314590454102, accy: 0.5\n",
            "iter: 840 loss: 1.5190110206604004, accy: 0.56\n",
            "iter: 850 loss: 1.4224368333816528, accy: 0.54\n",
            "iter: 860 loss: 1.5206915140151978, accy: 0.54\n",
            "iter: 870 loss: 1.4612311124801636, accy: 0.58\n",
            "iter: 880 loss: 1.5327435731887817, accy: 0.64\n",
            "iter: 890 loss: 1.5709549188613892, accy: 0.52\n",
            "iter: 900 loss: 1.6597224473953247, accy: 0.44\n",
            "iter: 910 loss: 1.5136607885360718, accy: 0.54\n",
            "iter: 920 loss: 1.6177421808242798, accy: 0.4\n",
            "iter: 930 loss: 1.5490140914916992, accy: 0.4\n",
            "iter: 940 loss: 1.565822958946228, accy: 0.48\n",
            "iter: 950 loss: 1.5879117250442505, accy: 0.42\n",
            "iter: 960 loss: 1.7167340517044067, accy: 0.42\n",
            "iter: 970 loss: 1.5599933862686157, accy: 0.52\n",
            "iter: 980 loss: 1.5121279954910278, accy: 0.58\n",
            "iter: 990 loss: 1.5675829648971558, accy: 0.5\n",
            "iter: 1000 loss: 1.5127089023590088, accy: 0.56\n",
            "iter: 1010 loss: 1.3462138175964355, accy: 0.66\n",
            "iter: 1020 loss: 1.4513885974884033, accy: 0.5\n",
            "iter: 1030 loss: 1.3942233324050903, accy: 0.62\n",
            "iter: 1040 loss: 1.425327181816101, accy: 0.52\n",
            "iter: 1050 loss: 1.5324583053588867, accy: 0.54\n",
            "iter: 1060 loss: 1.4101040363311768, accy: 0.52\n",
            "iter: 1070 loss: 1.2365024089813232, accy: 0.66\n",
            "iter: 1080 loss: 1.3184781074523926, accy: 0.66\n",
            "iter: 1090 loss: 1.4169163703918457, accy: 0.6\n",
            "iter: 1100 loss: 1.3440874814987183, accy: 0.64\n",
            "iter: 1110 loss: 1.311629056930542, accy: 0.62\n",
            "iter: 1120 loss: 1.4684193134307861, accy: 0.56\n",
            "iter: 1130 loss: 1.3417177200317383, accy: 0.56\n",
            "iter: 1140 loss: 1.3219527006149292, accy: 0.62\n",
            "iter: 1150 loss: 1.3502215147018433, accy: 0.66\n",
            "iter: 1160 loss: 1.4779174327850342, accy: 0.52\n",
            "iter: 1170 loss: 1.4825280904769897, accy: 0.4\n",
            "iter: 1180 loss: 1.4814375638961792, accy: 0.48\n",
            "iter: 1190 loss: 1.2823811769485474, accy: 0.7\n",
            "iter: 1200 loss: 1.4338244199752808, accy: 0.56\n",
            "iter: 1210 loss: 1.4861903190612793, accy: 0.54\n",
            "iter: 1220 loss: 1.4331260919570923, accy: 0.54\n",
            "iter: 1230 loss: 1.3573623895645142, accy: 0.58\n",
            "iter: 1240 loss: 1.495927095413208, accy: 0.42\n",
            "iter: 1250 loss: 1.3273907899856567, accy: 0.64\n",
            "iter: 1260 loss: 1.4709891080856323, accy: 0.48\n",
            "iter: 1270 loss: 1.357014775276184, accy: 0.52\n",
            "iter: 1280 loss: 1.3656911849975586, accy: 0.54\n",
            "iter: 1290 loss: 1.3163812160491943, accy: 0.62\n",
            "iter: 1300 loss: 1.5128711462020874, accy: 0.46\n",
            "iter: 1310 loss: 1.5495593547821045, accy: 0.48\n",
            "iter: 1320 loss: 1.493929386138916, accy: 0.54\n",
            "iter: 1330 loss: 1.2899364233016968, accy: 0.52\n",
            "iter: 1340 loss: 1.3344987630844116, accy: 0.54\n",
            "iter: 1350 loss: 1.3721373081207275, accy: 0.56\n",
            "iter: 1360 loss: 1.3751792907714844, accy: 0.56\n",
            "iter: 1370 loss: 1.4033503532409668, accy: 0.44\n",
            "iter: 1380 loss: 1.255352258682251, accy: 0.72\n",
            "iter: 1390 loss: 1.2726892232894897, accy: 0.56\n",
            "iter: 1400 loss: 1.3060873746871948, accy: 0.58\n",
            "iter: 1410 loss: 1.3943531513214111, accy: 0.5\n",
            "iter: 1420 loss: 1.2785731554031372, accy: 0.62\n",
            "iter: 1430 loss: 1.4295653104782104, accy: 0.6\n",
            "iter: 1440 loss: 1.36232328414917, accy: 0.58\n",
            "iter: 1450 loss: 1.3162661790847778, accy: 0.58\n",
            "iter: 1460 loss: 1.310430645942688, accy: 0.6\n",
            "iter: 1470 loss: 1.325499415397644, accy: 0.68\n",
            "iter: 1480 loss: 1.4730557203292847, accy: 0.52\n",
            "iter: 1490 loss: 1.1915725469589233, accy: 0.66\n",
            "iter: 1500 loss: 1.3639936447143555, accy: 0.54\n",
            "iter: 1510 loss: 1.2793041467666626, accy: 0.58\n",
            "iter: 1520 loss: 1.4851166009902954, accy: 0.54\n",
            "iter: 1530 loss: 1.3852823972702026, accy: 0.62\n",
            "iter: 1540 loss: 1.5695511102676392, accy: 0.48\n",
            "iter: 1550 loss: 1.284367322921753, accy: 0.66\n",
            "iter: 1560 loss: 1.4833935499191284, accy: 0.5\n",
            "iter: 1570 loss: 1.374139666557312, accy: 0.56\n",
            "iter: 1580 loss: 1.3035372495651245, accy: 0.56\n",
            "iter: 1590 loss: 1.2520383596420288, accy: 0.64\n",
            "iter: 1600 loss: 1.520236611366272, accy: 0.5\n",
            "iter: 1610 loss: 1.3701963424682617, accy: 0.52\n",
            "iter: 1620 loss: 1.5035327672958374, accy: 0.46\n",
            "iter: 1630 loss: 1.2291896343231201, accy: 0.7\n",
            "iter: 1640 loss: 1.3228586912155151, accy: 0.58\n",
            "iter: 1650 loss: 1.2318869829177856, accy: 0.66\n",
            "iter: 1660 loss: 1.3906697034835815, accy: 0.52\n",
            "iter: 1670 loss: 1.4389582872390747, accy: 0.48\n",
            "iter: 1680 loss: 1.3995274305343628, accy: 0.48\n",
            "iter: 1690 loss: 1.3562524318695068, accy: 0.58\n",
            "iter: 1700 loss: 1.247818946838379, accy: 0.6\n",
            "iter: 1710 loss: 1.3256232738494873, accy: 0.56\n",
            "iter: 1720 loss: 1.1779495477676392, accy: 0.64\n",
            "iter: 1730 loss: 1.0518990755081177, accy: 0.8\n",
            "iter: 1740 loss: 1.2969720363616943, accy: 0.58\n",
            "iter: 1750 loss: 1.3182579278945923, accy: 0.58\n",
            "iter: 1760 loss: 1.4965165853500366, accy: 0.52\n",
            "iter: 1770 loss: 1.3161135911941528, accy: 0.6\n",
            "iter: 1780 loss: 1.3958474397659302, accy: 0.5\n",
            "iter: 1790 loss: 1.1339466571807861, accy: 0.66\n",
            "iter: 1800 loss: 1.174311637878418, accy: 0.7\n",
            "iter: 1810 loss: 1.191709280014038, accy: 0.6\n",
            "iter: 1820 loss: 1.1869730949401855, accy: 0.66\n",
            "iter: 1830 loss: 1.5098860263824463, accy: 0.48\n",
            "iter: 1840 loss: 1.192022442817688, accy: 0.58\n",
            "iter: 1850 loss: 1.4625611305236816, accy: 0.5\n",
            "iter: 1860 loss: 1.414141058921814, accy: 0.46\n",
            "iter: 1870 loss: 1.3835338354110718, accy: 0.58\n",
            "iter: 1880 loss: 1.2962247133255005, accy: 0.56\n",
            "iter: 1890 loss: 1.1587605476379395, accy: 0.6\n",
            "iter: 1900 loss: 1.3605831861495972, accy: 0.64\n",
            "iter: 1910 loss: 1.2434475421905518, accy: 0.62\n",
            "iter: 1920 loss: 1.2914931774139404, accy: 0.56\n",
            "iter: 1930 loss: 1.3041551113128662, accy: 0.56\n",
            "iter: 1940 loss: 1.2006229162216187, accy: 0.6\n",
            "iter: 1950 loss: 1.3102701902389526, accy: 0.46\n",
            "iter: 1960 loss: 1.301945686340332, accy: 0.64\n",
            "iter: 1970 loss: 1.358770489692688, accy: 0.54\n",
            "iter: 1980 loss: 1.1292804479599, accy: 0.62\n",
            "iter: 1990 loss: 1.1634002923965454, accy: 0.56\n",
            "iter: 2000 loss: 1.230259895324707, accy: 0.62\n",
            "iter: 2010 loss: 1.302597999572754, accy: 0.5\n",
            "iter: 2020 loss: 1.2620598077774048, accy: 0.58\n",
            "iter: 2030 loss: 1.1263256072998047, accy: 0.68\n",
            "iter: 2040 loss: 1.1743121147155762, accy: 0.7\n",
            "iter: 2050 loss: 1.1997342109680176, accy: 0.66\n",
            "iter: 2060 loss: 1.4132096767425537, accy: 0.6\n",
            "iter: 2070 loss: 1.327769160270691, accy: 0.52\n",
            "iter: 2080 loss: 1.2435948848724365, accy: 0.6\n",
            "iter: 2090 loss: 1.0072433948516846, accy: 0.74\n",
            "iter: 2100 loss: 1.1688488721847534, accy: 0.68\n",
            "iter: 2110 loss: 1.1546745300292969, accy: 0.68\n",
            "iter: 2120 loss: 1.1256215572357178, accy: 0.66\n",
            "iter: 2130 loss: 1.2427566051483154, accy: 0.58\n",
            "iter: 2140 loss: 1.3627585172653198, accy: 0.58\n",
            "iter: 2150 loss: 1.2605549097061157, accy: 0.6\n",
            "iter: 2160 loss: 1.1076250076293945, accy: 0.6\n",
            "iter: 2170 loss: 1.2280693054199219, accy: 0.62\n",
            "iter: 2180 loss: 1.1750558614730835, accy: 0.6\n",
            "iter: 2190 loss: 1.1158382892608643, accy: 0.64\n",
            "iter: 2200 loss: 1.3536615371704102, accy: 0.6\n",
            "iter: 2210 loss: 1.3602253198623657, accy: 0.56\n",
            "iter: 2220 loss: 1.0731260776519775, accy: 0.62\n",
            "iter: 2230 loss: 1.1655950546264648, accy: 0.64\n",
            "iter: 2240 loss: 1.1682389974594116, accy: 0.6\n",
            "iter: 2250 loss: 1.1487256288528442, accy: 0.64\n",
            "iter: 2260 loss: 1.1542634963989258, accy: 0.66\n",
            "iter: 2270 loss: 0.9986863732337952, accy: 0.66\n",
            "iter: 2280 loss: 1.3211628198623657, accy: 0.58\n",
            "iter: 2290 loss: 1.228746771812439, accy: 0.62\n",
            "iter: 2300 loss: 1.1640583276748657, accy: 0.62\n",
            "iter: 2310 loss: 1.3032755851745605, accy: 0.48\n",
            "iter: 2320 loss: 1.2515113353729248, accy: 0.6\n",
            "iter: 2330 loss: 1.2289414405822754, accy: 0.64\n",
            "iter: 2340 loss: 1.2716279029846191, accy: 0.52\n",
            "iter: 2350 loss: 1.1717277765274048, accy: 0.58\n",
            "iter: 2360 loss: 1.1158514022827148, accy: 0.68\n",
            "iter: 2370 loss: 1.1515651941299438, accy: 0.64\n",
            "iter: 2380 loss: 1.6037219762802124, accy: 0.5\n",
            "iter: 2390 loss: 1.2785130739212036, accy: 0.64\n",
            "iter: 2400 loss: 1.111100196838379, accy: 0.68\n",
            "iter: 2410 loss: 1.1680783033370972, accy: 0.64\n",
            "iter: 2420 loss: 1.159449815750122, accy: 0.68\n",
            "iter: 2430 loss: 1.073801875114441, accy: 0.64\n",
            "iter: 2440 loss: 1.1910878419876099, accy: 0.64\n",
            "iter: 2450 loss: 1.184628963470459, accy: 0.52\n",
            "iter: 2460 loss: 1.2140179872512817, accy: 0.6\n",
            "iter: 2470 loss: 1.002898097038269, accy: 0.66\n",
            "iter: 2480 loss: 0.9737938046455383, accy: 0.72\n",
            "iter: 2490 loss: 1.0138936042785645, accy: 0.68\n",
            "iter: 2500 loss: 1.2277956008911133, accy: 0.58\n",
            "iter: 2510 loss: 1.2069298028945923, accy: 0.58\n",
            "iter: 2520 loss: 1.2978415489196777, accy: 0.54\n",
            "iter: 2530 loss: 1.1340875625610352, accy: 0.58\n",
            "iter: 2540 loss: 1.0737226009368896, accy: 0.62\n",
            "iter: 2550 loss: 1.234665870666504, accy: 0.62\n",
            "iter: 2560 loss: 1.10348641872406, accy: 0.72\n",
            "iter: 2570 loss: 1.046981930732727, accy: 0.7\n",
            "iter: 2580 loss: 1.1676616668701172, accy: 0.52\n",
            "iter: 2590 loss: 1.1180310249328613, accy: 0.64\n",
            "iter: 2600 loss: 1.193364143371582, accy: 0.62\n",
            "iter: 2610 loss: 0.9810368418693542, accy: 0.76\n",
            "iter: 2620 loss: 1.2056328058242798, accy: 0.62\n",
            "iter: 2630 loss: 1.185665249824524, accy: 0.68\n",
            "iter: 2640 loss: 1.0361791849136353, accy: 0.6\n",
            "iter: 2650 loss: 1.1068137884140015, accy: 0.68\n",
            "iter: 2660 loss: 1.0371952056884766, accy: 0.68\n",
            "iter: 2670 loss: 0.9624031782150269, accy: 0.76\n",
            "iter: 2680 loss: 1.186797857284546, accy: 0.64\n",
            "iter: 2690 loss: 1.1979469060897827, accy: 0.58\n",
            "iter: 2700 loss: 1.0303318500518799, accy: 0.68\n",
            "iter: 2710 loss: 1.2500423192977905, accy: 0.64\n",
            "iter: 2720 loss: 1.2911887168884277, accy: 0.52\n",
            "iter: 2730 loss: 1.1843810081481934, accy: 0.58\n",
            "iter: 2740 loss: 1.0313823223114014, accy: 0.66\n",
            "iter: 2750 loss: 1.0369749069213867, accy: 0.7\n",
            "iter: 2760 loss: 1.0807377099990845, accy: 0.6\n",
            "iter: 2770 loss: 1.074588656425476, accy: 0.64\n",
            "iter: 2780 loss: 1.1316866874694824, accy: 0.68\n",
            "iter: 2790 loss: 1.0402803421020508, accy: 0.68\n",
            "iter: 2800 loss: 0.882142186164856, accy: 0.72\n",
            "iter: 2810 loss: 1.0260847806930542, accy: 0.74\n",
            "iter: 2820 loss: 1.145065188407898, accy: 0.64\n",
            "iter: 2830 loss: 0.9327945709228516, accy: 0.64\n",
            "iter: 2840 loss: 0.931044340133667, accy: 0.68\n",
            "iter: 2850 loss: 1.036800742149353, accy: 0.7\n",
            "iter: 2860 loss: 1.2837251424789429, accy: 0.62\n",
            "iter: 2870 loss: 0.9898697733879089, accy: 0.7\n",
            "iter: 2880 loss: 0.9623242020606995, accy: 0.74\n",
            "iter: 2890 loss: 1.0429950952529907, accy: 0.66\n",
            "iter: 2900 loss: 1.0282362699508667, accy: 0.64\n",
            "iter: 2910 loss: 1.0424675941467285, accy: 0.7\n",
            "iter: 2920 loss: 1.24392831325531, accy: 0.66\n",
            "iter: 2930 loss: 1.0424399375915527, accy: 0.64\n",
            "iter: 2940 loss: 1.1514561176300049, accy: 0.68\n",
            "iter: 2950 loss: 1.2730859518051147, accy: 0.64\n",
            "iter: 2960 loss: 1.1818573474884033, accy: 0.66\n",
            "iter: 2970 loss: 1.1805459260940552, accy: 0.66\n",
            "iter: 2980 loss: 1.1013939380645752, accy: 0.6\n",
            "iter: 2990 loss: 1.319169521331787, accy: 0.46\n",
            "iter: 3000 loss: 0.9438651204109192, accy: 0.72\n",
            "iter: 3010 loss: 0.9473890066146851, accy: 0.74\n",
            "iter: 3020 loss: 1.1185847520828247, accy: 0.66\n",
            "iter: 3030 loss: 0.8540589213371277, accy: 0.78\n",
            "iter: 3040 loss: 1.1505515575408936, accy: 0.6\n",
            "iter: 3050 loss: 0.9036712050437927, accy: 0.78\n",
            "iter: 3060 loss: 1.0240365266799927, accy: 0.7\n",
            "iter: 3070 loss: 1.1471710205078125, accy: 0.64\n",
            "iter: 3080 loss: 0.9625504016876221, accy: 0.78\n",
            "iter: 3090 loss: 1.251497745513916, accy: 0.6\n",
            "iter: 3100 loss: 0.9642862677574158, accy: 0.66\n",
            "iter: 3110 loss: 0.9106512665748596, accy: 0.74\n",
            "iter: 3120 loss: 1.2991582155227661, accy: 0.52\n",
            "iter: 3130 loss: 1.1735522747039795, accy: 0.64\n",
            "iter: 3140 loss: 1.2001638412475586, accy: 0.6\n",
            "iter: 3150 loss: 1.0032212734222412, accy: 0.7\n",
            "iter: 3160 loss: 1.4185580015182495, accy: 0.62\n",
            "iter: 3170 loss: 1.162087082862854, accy: 0.58\n",
            "iter: 3180 loss: 0.9975223541259766, accy: 0.66\n",
            "iter: 3190 loss: 1.382522463798523, accy: 0.66\n",
            "iter: 3200 loss: 1.3219501972198486, accy: 0.52\n",
            "iter: 3210 loss: 0.8931418657302856, accy: 0.74\n",
            "iter: 3220 loss: 1.1401662826538086, accy: 0.62\n",
            "iter: 3230 loss: 1.1344701051712036, accy: 0.64\n",
            "iter: 3240 loss: 1.0680286884307861, accy: 0.68\n",
            "iter: 3250 loss: 1.244455337524414, accy: 0.68\n",
            "iter: 3260 loss: 1.069058895111084, accy: 0.62\n",
            "iter: 3270 loss: 0.9647605419158936, accy: 0.64\n",
            "iter: 3280 loss: 1.1319231986999512, accy: 0.62\n",
            "iter: 3290 loss: 1.0079020261764526, accy: 0.68\n",
            "iter: 3300 loss: 1.0751159191131592, accy: 0.58\n",
            "iter: 3310 loss: 0.9852150678634644, accy: 0.64\n",
            "iter: 3320 loss: 1.1272644996643066, accy: 0.58\n",
            "iter: 3330 loss: 1.3010809421539307, accy: 0.56\n",
            "iter: 3340 loss: 0.9599893689155579, accy: 0.7\n",
            "iter: 3350 loss: 0.9669557809829712, accy: 0.66\n",
            "iter: 3360 loss: 1.1239941120147705, accy: 0.68\n",
            "iter: 3370 loss: 1.0920790433883667, accy: 0.6\n",
            "iter: 3380 loss: 0.8588384389877319, accy: 0.76\n",
            "iter: 3390 loss: 1.0952351093292236, accy: 0.62\n",
            "iter: 3400 loss: 0.8417006731033325, accy: 0.78\n",
            "iter: 3410 loss: 0.9333052039146423, accy: 0.72\n",
            "iter: 3420 loss: 1.0201594829559326, accy: 0.68\n",
            "iter: 3430 loss: 1.3665236234664917, accy: 0.5\n",
            "iter: 3440 loss: 0.9877451062202454, accy: 0.62\n",
            "iter: 3450 loss: 1.1601245403289795, accy: 0.64\n",
            "iter: 3460 loss: 0.9869422316551208, accy: 0.68\n",
            "iter: 3470 loss: 0.9047262072563171, accy: 0.74\n",
            "iter: 3480 loss: 1.1301263570785522, accy: 0.7\n",
            "iter: 3490 loss: 0.998126208782196, accy: 0.64\n",
            "iter: 3500 loss: 0.9734695553779602, accy: 0.7\n",
            "iter: 3510 loss: 1.162459373474121, accy: 0.62\n",
            "iter: 3520 loss: 0.9013247489929199, accy: 0.76\n",
            "iter: 3530 loss: 1.0111461877822876, accy: 0.72\n",
            "iter: 3540 loss: 1.004608392715454, accy: 0.64\n",
            "iter: 3550 loss: 0.9150943756103516, accy: 0.72\n",
            "iter: 3560 loss: 0.8499584794044495, accy: 0.8\n",
            "iter: 3570 loss: 0.9243299961090088, accy: 0.8\n",
            "iter: 3580 loss: 1.2284345626831055, accy: 0.58\n",
            "iter: 3590 loss: 1.042542576789856, accy: 0.58\n",
            "iter: 3600 loss: 0.9151339530944824, accy: 0.74\n",
            "iter: 3610 loss: 0.9351605772972107, accy: 0.66\n",
            "iter: 3620 loss: 1.1360551118850708, accy: 0.7\n",
            "iter: 3630 loss: 1.0363056659698486, accy: 0.74\n",
            "iter: 3640 loss: 1.0492994785308838, accy: 0.58\n",
            "iter: 3650 loss: 1.1226322650909424, accy: 0.62\n",
            "iter: 3660 loss: 1.1017003059387207, accy: 0.62\n",
            "iter: 3670 loss: 1.163496494293213, accy: 0.52\n",
            "iter: 3680 loss: 1.1782691478729248, accy: 0.66\n",
            "iter: 3690 loss: 0.8777874112129211, accy: 0.76\n",
            "iter: 3700 loss: 1.2543423175811768, accy: 0.62\n",
            "iter: 3710 loss: 0.8328856825828552, accy: 0.74\n",
            "iter: 3720 loss: 1.1832165718078613, accy: 0.62\n",
            "iter: 3730 loss: 0.968244194984436, accy: 0.74\n",
            "iter: 3740 loss: 0.9978801608085632, accy: 0.74\n",
            "iter: 3750 loss: 0.8597183227539062, accy: 0.7\n",
            "iter: 3760 loss: 1.2011581659317017, accy: 0.58\n",
            "iter: 3770 loss: 0.9056555032730103, accy: 0.7\n",
            "iter: 3780 loss: 1.0484704971313477, accy: 0.7\n",
            "iter: 3790 loss: 1.0500162839889526, accy: 0.68\n",
            "iter: 3800 loss: 0.9361815452575684, accy: 0.7\n",
            "iter: 3810 loss: 0.9581881761550903, accy: 0.74\n",
            "iter: 3820 loss: 1.0222885608673096, accy: 0.64\n",
            "iter: 3830 loss: 1.0629850625991821, accy: 0.7\n",
            "iter: 3840 loss: 0.9770869612693787, accy: 0.68\n",
            "iter: 3850 loss: 1.0243773460388184, accy: 0.64\n",
            "iter: 3860 loss: 1.1909842491149902, accy: 0.6\n",
            "iter: 3870 loss: 1.0082017183303833, accy: 0.68\n",
            "iter: 3880 loss: 0.8897960782051086, accy: 0.74\n",
            "iter: 3890 loss: 1.0142321586608887, accy: 0.7\n",
            "iter: 3900 loss: 0.8553558588027954, accy: 0.76\n",
            "iter: 3910 loss: 0.7830981612205505, accy: 0.76\n",
            "iter: 3920 loss: 1.2015151977539062, accy: 0.7\n",
            "iter: 3930 loss: 0.8519530296325684, accy: 0.78\n",
            "iter: 3940 loss: 1.0119045972824097, accy: 0.66\n",
            "iter: 3950 loss: 0.8856609463691711, accy: 0.68\n",
            "iter: 3960 loss: 0.9445449709892273, accy: 0.7\n",
            "iter: 3970 loss: 0.9493868350982666, accy: 0.64\n",
            "iter: 3980 loss: 1.0004972219467163, accy: 0.66\n",
            "iter: 3990 loss: 0.8266383409500122, accy: 0.74\n",
            "iter: 4000 loss: 0.9179150462150574, accy: 0.68\n",
            "iter: 4010 loss: 1.162459135055542, accy: 0.66\n",
            "iter: 4020 loss: 1.1946780681610107, accy: 0.58\n",
            "iter: 4030 loss: 0.9439910650253296, accy: 0.72\n",
            "iter: 4040 loss: 1.1089528799057007, accy: 0.7\n",
            "iter: 4050 loss: 0.8958724737167358, accy: 0.68\n",
            "iter: 4060 loss: 1.1106972694396973, accy: 0.5\n",
            "iter: 4070 loss: 0.909684419631958, accy: 0.74\n",
            "iter: 4080 loss: 0.9496448040008545, accy: 0.7\n",
            "iter: 4090 loss: 0.9698096513748169, accy: 0.74\n",
            "iter: 4100 loss: 0.9603474140167236, accy: 0.6\n",
            "iter: 4110 loss: 0.9632432460784912, accy: 0.72\n",
            "iter: 4120 loss: 1.1003544330596924, accy: 0.6\n",
            "iter: 4130 loss: 0.896574079990387, accy: 0.74\n",
            "iter: 4140 loss: 1.0204112529754639, accy: 0.6\n",
            "iter: 4150 loss: 1.2012568712234497, accy: 0.62\n",
            "iter: 4160 loss: 1.0011485815048218, accy: 0.74\n",
            "iter: 4170 loss: 0.9396615624427795, accy: 0.64\n",
            "iter: 4180 loss: 0.8848206996917725, accy: 0.74\n",
            "iter: 4190 loss: 0.784363329410553, accy: 0.74\n",
            "iter: 4200 loss: 0.9694491624832153, accy: 0.7\n",
            "iter: 4210 loss: 0.9186546206474304, accy: 0.74\n",
            "iter: 4220 loss: 1.1002366542816162, accy: 0.62\n",
            "iter: 4230 loss: 1.038520097732544, accy: 0.58\n",
            "iter: 4240 loss: 1.0598640441894531, accy: 0.66\n",
            "iter: 4250 loss: 0.9577491879463196, accy: 0.74\n",
            "iter: 4260 loss: 1.0513666868209839, accy: 0.76\n",
            "iter: 4270 loss: 0.9465068578720093, accy: 0.7\n",
            "iter: 4280 loss: 0.9906362891197205, accy: 0.66\n",
            "iter: 4290 loss: 0.9870247840881348, accy: 0.64\n",
            "iter: 4300 loss: 0.9020549654960632, accy: 0.7\n",
            "iter: 4310 loss: 0.9968706369400024, accy: 0.66\n",
            "iter: 4320 loss: 0.8575193881988525, accy: 0.68\n",
            "iter: 4330 loss: 0.9136440753936768, accy: 0.68\n",
            "iter: 4340 loss: 0.9130271673202515, accy: 0.68\n",
            "iter: 4350 loss: 1.0244821310043335, accy: 0.64\n",
            "iter: 4360 loss: 0.938916027545929, accy: 0.68\n",
            "iter: 4370 loss: 0.7649459838867188, accy: 0.8\n",
            "iter: 4380 loss: 1.1136078834533691, accy: 0.62\n",
            "iter: 4390 loss: 1.2346967458724976, accy: 0.64\n",
            "iter: 4400 loss: 0.8811848163604736, accy: 0.66\n",
            "iter: 4410 loss: 1.0595791339874268, accy: 0.66\n",
            "iter: 4420 loss: 0.8163702487945557, accy: 0.7\n",
            "iter: 4430 loss: 0.8696258068084717, accy: 0.66\n",
            "iter: 4440 loss: 1.022955060005188, accy: 0.66\n",
            "iter: 4450 loss: 1.028193473815918, accy: 0.56\n",
            "iter: 4460 loss: 1.3529267311096191, accy: 0.58\n",
            "iter: 4470 loss: 1.1062018871307373, accy: 0.68\n",
            "iter: 4480 loss: 1.0042954683303833, accy: 0.7\n",
            "iter: 4490 loss: 0.9040981531143188, accy: 0.72\n",
            "iter: 4500 loss: 0.9080613851547241, accy: 0.7\n",
            "iter: 4510 loss: 0.9491798281669617, accy: 0.64\n",
            "iter: 4520 loss: 0.8798004984855652, accy: 0.78\n",
            "iter: 4530 loss: 0.9707816243171692, accy: 0.76\n",
            "iter: 4540 loss: 0.9165661334991455, accy: 0.7\n",
            "iter: 4550 loss: 0.9374097585678101, accy: 0.74\n",
            "iter: 4560 loss: 1.0496071577072144, accy: 0.76\n",
            "iter: 4570 loss: 1.0184357166290283, accy: 0.74\n",
            "iter: 4580 loss: 1.1877007484436035, accy: 0.7\n",
            "iter: 4590 loss: 0.8764745593070984, accy: 0.7\n",
            "iter: 4600 loss: 0.8787056803703308, accy: 0.76\n",
            "iter: 4610 loss: 0.9246477484703064, accy: 0.72\n",
            "iter: 4620 loss: 1.129257082939148, accy: 0.58\n",
            "iter: 4630 loss: 0.9652211666107178, accy: 0.68\n",
            "iter: 4640 loss: 1.0887773036956787, accy: 0.64\n",
            "iter: 4650 loss: 1.1510854959487915, accy: 0.58\n",
            "iter: 4660 loss: 0.761457622051239, accy: 0.74\n",
            "iter: 4670 loss: 0.8990163207054138, accy: 0.74\n",
            "iter: 4680 loss: 0.9099793434143066, accy: 0.74\n",
            "iter: 4690 loss: 1.0615808963775635, accy: 0.58\n",
            "iter: 4700 loss: 0.949469804763794, accy: 0.66\n",
            "iter: 4710 loss: 1.122701644897461, accy: 0.54\n",
            "iter: 4720 loss: 1.1066012382507324, accy: 0.58\n",
            "iter: 4730 loss: 0.8769400715827942, accy: 0.66\n",
            "iter: 4740 loss: 0.7925411462783813, accy: 0.78\n",
            "iter: 4750 loss: 0.8577057123184204, accy: 0.7\n",
            "iter: 4760 loss: 0.8929595351219177, accy: 0.74\n",
            "iter: 4770 loss: 0.8103653788566589, accy: 0.78\n",
            "iter: 4780 loss: 0.7126187086105347, accy: 0.8\n",
            "iter: 4790 loss: 1.0715913772583008, accy: 0.7\n",
            "iter: 4800 loss: 0.8079618811607361, accy: 0.78\n",
            "iter: 4810 loss: 0.9238847494125366, accy: 0.78\n",
            "iter: 4820 loss: 0.8552111983299255, accy: 0.72\n",
            "iter: 4830 loss: 0.894559919834137, accy: 0.74\n",
            "iter: 4840 loss: 0.9262425303459167, accy: 0.62\n",
            "iter: 4850 loss: 0.8833138942718506, accy: 0.66\n",
            "iter: 4860 loss: 1.0031872987747192, accy: 0.64\n",
            "iter: 4870 loss: 1.1796455383300781, accy: 0.62\n",
            "iter: 4880 loss: 0.9519032835960388, accy: 0.7\n",
            "iter: 4890 loss: 0.9091711640357971, accy: 0.66\n",
            "iter: 4900 loss: 0.9232621788978577, accy: 0.7\n",
            "iter: 4910 loss: 0.9189333319664001, accy: 0.7\n",
            "iter: 4920 loss: 1.107053518295288, accy: 0.62\n",
            "iter: 4930 loss: 0.9175309538841248, accy: 0.7\n",
            "iter: 4940 loss: 0.9695742130279541, accy: 0.7\n",
            "iter: 4950 loss: 0.9882493019104004, accy: 0.72\n",
            "iter: 4960 loss: 0.9940619468688965, accy: 0.64\n",
            "iter: 4970 loss: 0.830319344997406, accy: 0.76\n",
            "iter: 4980 loss: 1.0668892860412598, accy: 0.54\n",
            "iter: 4990 loss: 1.0741859674453735, accy: 0.68\n",
            "iter: 5000 loss: 1.0451871156692505, accy: 0.66\n",
            "iter: 5010 loss: 1.0377575159072876, accy: 0.6\n",
            "iter: 5020 loss: 1.0368897914886475, accy: 0.58\n",
            "iter: 5030 loss: 0.8767349123954773, accy: 0.7\n",
            "iter: 5040 loss: 0.8213326930999756, accy: 0.68\n",
            "iter: 5050 loss: 0.8508589863777161, accy: 0.78\n",
            "iter: 5060 loss: 0.9255436062812805, accy: 0.7\n",
            "iter: 5070 loss: 0.8196253776550293, accy: 0.7\n",
            "iter: 5080 loss: 0.8525593280792236, accy: 0.76\n",
            "iter: 5090 loss: 1.0715157985687256, accy: 0.66\n",
            "iter: 5100 loss: 1.1259112358093262, accy: 0.66\n",
            "iter: 5110 loss: 0.9528200030326843, accy: 0.76\n",
            "iter: 5120 loss: 0.8963607549667358, accy: 0.76\n",
            "iter: 5130 loss: 0.9121382236480713, accy: 0.76\n",
            "iter: 5140 loss: 0.7814764380455017, accy: 0.78\n",
            "iter: 5150 loss: 1.037380337715149, accy: 0.66\n",
            "iter: 5160 loss: 0.795991837978363, accy: 0.72\n",
            "iter: 5170 loss: 0.9239375591278076, accy: 0.64\n",
            "iter: 5180 loss: 0.8382396697998047, accy: 0.74\n",
            "iter: 5190 loss: 0.9366079568862915, accy: 0.64\n",
            "iter: 5200 loss: 0.7059154510498047, accy: 0.82\n",
            "iter: 5210 loss: 0.8739340305328369, accy: 0.76\n",
            "iter: 5220 loss: 0.9742763638496399, accy: 0.66\n",
            "iter: 5230 loss: 0.9933690428733826, accy: 0.72\n",
            "iter: 5240 loss: 0.9647209644317627, accy: 0.62\n",
            "iter: 5250 loss: 0.999855101108551, accy: 0.7\n",
            "iter: 5260 loss: 0.8501676917076111, accy: 0.66\n",
            "iter: 5270 loss: 0.9161982536315918, accy: 0.7\n",
            "iter: 5280 loss: 1.0017110109329224, accy: 0.66\n",
            "iter: 5290 loss: 0.7162315249443054, accy: 0.78\n",
            "iter: 5300 loss: 1.1938859224319458, accy: 0.62\n",
            "iter: 5310 loss: 0.9449568390846252, accy: 0.76\n",
            "iter: 5320 loss: 0.9802901744842529, accy: 0.74\n",
            "iter: 5330 loss: 0.9406421780586243, accy: 0.68\n",
            "iter: 5340 loss: 1.088275671005249, accy: 0.64\n",
            "iter: 5350 loss: 0.7753954529762268, accy: 0.8\n",
            "iter: 5360 loss: 0.7604309320449829, accy: 0.76\n",
            "iter: 5370 loss: 0.839048445224762, accy: 0.82\n",
            "iter: 5380 loss: 0.8832122087478638, accy: 0.74\n",
            "iter: 5390 loss: 1.110916256904602, accy: 0.64\n",
            "iter: 5400 loss: 0.7983079552650452, accy: 0.76\n",
            "iter: 5410 loss: 0.8380210399627686, accy: 0.74\n",
            "iter: 5420 loss: 1.0093138217926025, accy: 0.76\n",
            "iter: 5430 loss: 0.8833823204040527, accy: 0.68\n",
            "iter: 5440 loss: 0.9534228444099426, accy: 0.6\n",
            "iter: 5450 loss: 0.8036256432533264, accy: 0.66\n",
            "iter: 5460 loss: 1.0227807760238647, accy: 0.68\n",
            "iter: 5470 loss: 0.7276812195777893, accy: 0.76\n",
            "iter: 5480 loss: 1.0923867225646973, accy: 0.66\n",
            "iter: 5490 loss: 1.053066372871399, accy: 0.68\n",
            "iter: 5500 loss: 0.8469001054763794, accy: 0.66\n",
            "iter: 5510 loss: 0.9459332227706909, accy: 0.76\n",
            "iter: 5520 loss: 1.07867431640625, accy: 0.6\n",
            "iter: 5530 loss: 0.9255174994468689, accy: 0.72\n",
            "iter: 5540 loss: 0.8599715232849121, accy: 0.7\n",
            "iter: 5550 loss: 1.0757756233215332, accy: 0.68\n",
            "iter: 5560 loss: 1.2779817581176758, accy: 0.58\n",
            "iter: 5570 loss: 1.0931068658828735, accy: 0.64\n",
            "iter: 5580 loss: 0.8917017579078674, accy: 0.74\n",
            "iter: 5590 loss: 0.7036526203155518, accy: 0.76\n",
            "iter: 5600 loss: 0.929282009601593, accy: 0.72\n",
            "iter: 5610 loss: 1.0668487548828125, accy: 0.58\n",
            "iter: 5620 loss: 0.9997944831848145, accy: 0.68\n",
            "iter: 5630 loss: 0.840770423412323, accy: 0.76\n",
            "iter: 5640 loss: 0.8042879700660706, accy: 0.74\n",
            "iter: 5650 loss: 0.962357759475708, accy: 0.68\n",
            "iter: 5660 loss: 0.9054762125015259, accy: 0.7\n",
            "iter: 5670 loss: 0.8142549395561218, accy: 0.78\n",
            "iter: 5680 loss: 0.9445580840110779, accy: 0.66\n",
            "iter: 5690 loss: 0.7701295614242554, accy: 0.78\n",
            "iter: 5700 loss: 0.9316735863685608, accy: 0.68\n",
            "iter: 5710 loss: 1.0812931060791016, accy: 0.72\n",
            "iter: 5720 loss: 0.8486248850822449, accy: 0.72\n",
            "iter: 5730 loss: 0.8947741985321045, accy: 0.74\n",
            "iter: 5740 loss: 0.886655330657959, accy: 0.74\n",
            "iter: 5750 loss: 0.7735675573348999, accy: 0.64\n",
            "iter: 5760 loss: 0.9418163299560547, accy: 0.62\n",
            "iter: 5770 loss: 0.9198744893074036, accy: 0.72\n",
            "iter: 5780 loss: 1.0056442022323608, accy: 0.64\n",
            "iter: 5790 loss: 0.9002837538719177, accy: 0.74\n",
            "iter: 5800 loss: 0.8870365023612976, accy: 0.74\n",
            "iter: 5810 loss: 0.9601983428001404, accy: 0.68\n",
            "iter: 5820 loss: 0.8407243490219116, accy: 0.74\n",
            "iter: 5830 loss: 0.8840025067329407, accy: 0.8\n",
            "iter: 5840 loss: 1.195773959159851, accy: 0.6\n",
            "iter: 5850 loss: 0.7404261827468872, accy: 0.72\n",
            "iter: 5860 loss: 0.9275654554367065, accy: 0.62\n",
            "iter: 5870 loss: 0.6591340899467468, accy: 0.8\n",
            "iter: 5880 loss: 1.0556148290634155, accy: 0.58\n",
            "iter: 5890 loss: 0.8764351010322571, accy: 0.7\n",
            "iter: 5900 loss: 1.1088294982910156, accy: 0.68\n",
            "iter: 5910 loss: 0.9544548988342285, accy: 0.62\n",
            "iter: 5920 loss: 0.942160427570343, accy: 0.64\n",
            "iter: 5930 loss: 0.790488064289093, accy: 0.76\n",
            "iter: 5940 loss: 1.1841883659362793, accy: 0.58\n",
            "iter: 5950 loss: 0.968392014503479, accy: 0.74\n",
            "iter: 5960 loss: 0.7561398148536682, accy: 0.74\n",
            "iter: 5970 loss: 1.0471243858337402, accy: 0.64\n",
            "iter: 5980 loss: 0.8966619372367859, accy: 0.66\n",
            "iter: 5990 loss: 1.016971230506897, accy: 0.66\n",
            "iter: 6000 loss: 0.7033227682113647, accy: 0.78\n",
            "iter: 6010 loss: 0.7364771366119385, accy: 0.78\n",
            "iter: 6020 loss: 1.0970611572265625, accy: 0.64\n",
            "iter: 6030 loss: 0.875546395778656, accy: 0.72\n",
            "iter: 6040 loss: 0.8338630199432373, accy: 0.74\n",
            "iter: 6050 loss: 0.7890316843986511, accy: 0.66\n",
            "iter: 6060 loss: 0.7998149991035461, accy: 0.72\n",
            "iter: 6070 loss: 0.8692148327827454, accy: 0.68\n",
            "iter: 6080 loss: 0.8531813621520996, accy: 0.76\n",
            "iter: 6090 loss: 0.7061939835548401, accy: 0.8\n",
            "iter: 6100 loss: 0.8804609775543213, accy: 0.68\n",
            "iter: 6110 loss: 0.9621345400810242, accy: 0.72\n",
            "iter: 6120 loss: 0.663472056388855, accy: 0.8\n",
            "iter: 6130 loss: 1.123776912689209, accy: 0.72\n",
            "iter: 6140 loss: 1.171507716178894, accy: 0.6\n",
            "iter: 6150 loss: 1.3692231178283691, accy: 0.56\n",
            "iter: 6160 loss: 0.8443616628646851, accy: 0.7\n",
            "iter: 6170 loss: 1.0897326469421387, accy: 0.64\n",
            "iter: 6180 loss: 0.5287317037582397, accy: 0.82\n",
            "iter: 6190 loss: 0.8404584527015686, accy: 0.72\n",
            "iter: 6200 loss: 0.8298066854476929, accy: 0.64\n",
            "iter: 6210 loss: 0.9964892864227295, accy: 0.68\n",
            "iter: 6220 loss: 0.8592005968093872, accy: 0.72\n",
            "iter: 6230 loss: 0.9568163156509399, accy: 0.62\n",
            "iter: 6240 loss: 1.153446078300476, accy: 0.6\n",
            "iter: 6250 loss: 0.9993653893470764, accy: 0.64\n",
            "iter: 6260 loss: 0.8172958493232727, accy: 0.72\n",
            "iter: 6270 loss: 0.9576572179794312, accy: 0.66\n",
            "iter: 6280 loss: 1.1023470163345337, accy: 0.66\n",
            "iter: 6290 loss: 1.1986600160598755, accy: 0.6\n",
            "iter: 6300 loss: 0.8115531802177429, accy: 0.72\n",
            "iter: 6310 loss: 1.1401349306106567, accy: 0.68\n",
            "iter: 6320 loss: 1.1033729314804077, accy: 0.58\n",
            "iter: 6330 loss: 0.7230350375175476, accy: 0.74\n",
            "iter: 6340 loss: 0.744565486907959, accy: 0.8\n",
            "iter: 6350 loss: 0.9520829916000366, accy: 0.68\n",
            "iter: 6360 loss: 1.145443081855774, accy: 0.64\n",
            "iter: 6370 loss: 0.8096210360527039, accy: 0.72\n",
            "iter: 6380 loss: 0.9300556778907776, accy: 0.7\n",
            "iter: 6390 loss: 0.8537861108779907, accy: 0.76\n",
            "iter: 6400 loss: 0.6325774192810059, accy: 0.78\n",
            "iter: 6410 loss: 0.8269873261451721, accy: 0.74\n",
            "iter: 6420 loss: 0.7600051760673523, accy: 0.74\n",
            "iter: 6430 loss: 0.9448376297950745, accy: 0.7\n",
            "iter: 6440 loss: 0.7954341769218445, accy: 0.76\n",
            "iter: 6450 loss: 0.9115031957626343, accy: 0.72\n",
            "iter: 6460 loss: 1.219333529472351, accy: 0.62\n",
            "iter: 6470 loss: 1.0048738718032837, accy: 0.64\n",
            "iter: 6480 loss: 1.1729304790496826, accy: 0.62\n",
            "iter: 6490 loss: 0.8424388766288757, accy: 0.72\n",
            "iter: 6500 loss: 0.856813371181488, accy: 0.82\n",
            "iter: 6510 loss: 1.0365052223205566, accy: 0.68\n",
            "iter: 6520 loss: 0.9652290344238281, accy: 0.66\n",
            "iter: 6530 loss: 0.8577231764793396, accy: 0.72\n",
            "iter: 6540 loss: 0.8037910461425781, accy: 0.7\n",
            "iter: 6550 loss: 1.0347874164581299, accy: 0.66\n",
            "iter: 6560 loss: 0.8405573964118958, accy: 0.68\n",
            "iter: 6570 loss: 0.9305811524391174, accy: 0.68\n",
            "iter: 6580 loss: 0.6981983184814453, accy: 0.72\n",
            "iter: 6590 loss: 1.0333977937698364, accy: 0.66\n",
            "iter: 6600 loss: 0.9671247005462646, accy: 0.76\n",
            "iter: 6610 loss: 0.9265010356903076, accy: 0.64\n",
            "iter: 6620 loss: 0.7920715808868408, accy: 0.76\n",
            "iter: 6630 loss: 0.9525226354598999, accy: 0.66\n",
            "iter: 6640 loss: 0.7617509365081787, accy: 0.74\n",
            "iter: 6650 loss: 0.8063556551933289, accy: 0.84\n",
            "iter: 6660 loss: 0.8456683158874512, accy: 0.74\n",
            "iter: 6670 loss: 0.6571142673492432, accy: 0.82\n",
            "iter: 6680 loss: 0.7428850531578064, accy: 0.78\n",
            "iter: 6690 loss: 0.7665762901306152, accy: 0.72\n",
            "iter: 6700 loss: 0.7092408537864685, accy: 0.78\n",
            "iter: 6710 loss: 0.6782879829406738, accy: 0.8\n",
            "iter: 6720 loss: 0.9793076515197754, accy: 0.7\n",
            "iter: 6730 loss: 0.8265535235404968, accy: 0.68\n",
            "iter: 6740 loss: 0.6907685995101929, accy: 0.8\n",
            "iter: 6750 loss: 0.8676983714103699, accy: 0.76\n",
            "iter: 6760 loss: 0.9107388854026794, accy: 0.68\n",
            "iter: 6770 loss: 0.8517828583717346, accy: 0.62\n",
            "iter: 6780 loss: 1.0921130180358887, accy: 0.62\n",
            "iter: 6790 loss: 0.7530665397644043, accy: 0.74\n",
            "iter: 6800 loss: 0.6869009137153625, accy: 0.82\n",
            "iter: 6810 loss: 0.5905988216400146, accy: 0.82\n",
            "iter: 6820 loss: 0.9044485688209534, accy: 0.74\n",
            "iter: 6830 loss: 1.0422853231430054, accy: 0.6\n",
            "iter: 6840 loss: 0.7237199544906616, accy: 0.78\n",
            "iter: 6850 loss: 0.8091270327568054, accy: 0.66\n",
            "iter: 6860 loss: 0.7785100340843201, accy: 0.7\n",
            "iter: 6870 loss: 0.8695560693740845, accy: 0.8\n",
            "iter: 6880 loss: 0.778766930103302, accy: 0.7\n",
            "iter: 6890 loss: 0.6352967023849487, accy: 0.86\n",
            "iter: 6900 loss: 0.8247420787811279, accy: 0.7\n",
            "iter: 6910 loss: 0.866710901260376, accy: 0.72\n",
            "iter: 6920 loss: 0.9611278772354126, accy: 0.62\n",
            "iter: 6930 loss: 1.0581941604614258, accy: 0.58\n",
            "iter: 6940 loss: 1.062703251838684, accy: 0.64\n",
            "iter: 6950 loss: 0.83428955078125, accy: 0.74\n",
            "iter: 6960 loss: 0.7946714758872986, accy: 0.72\n",
            "iter: 6970 loss: 1.0452873706817627, accy: 0.68\n",
            "iter: 6980 loss: 1.056026816368103, accy: 0.66\n",
            "iter: 6990 loss: 0.8231114149093628, accy: 0.8\n",
            "iter: 7000 loss: 1.2109389305114746, accy: 0.66\n",
            "iter: 7010 loss: 0.892092227935791, accy: 0.72\n",
            "iter: 7020 loss: 0.7029466032981873, accy: 0.78\n",
            "iter: 7030 loss: 0.9418992400169373, accy: 0.66\n",
            "iter: 7040 loss: 0.9083698391914368, accy: 0.78\n",
            "iter: 7050 loss: 1.0671522617340088, accy: 0.68\n",
            "iter: 7060 loss: 1.010464072227478, accy: 0.6\n",
            "iter: 7070 loss: 0.9344809055328369, accy: 0.68\n",
            "iter: 7080 loss: 0.7208549380302429, accy: 0.72\n",
            "iter: 7090 loss: 0.9839709997177124, accy: 0.56\n",
            "iter: 7100 loss: 0.9516498446464539, accy: 0.72\n",
            "iter: 7110 loss: 0.7798351049423218, accy: 0.72\n",
            "iter: 7120 loss: 0.7863837480545044, accy: 0.74\n",
            "iter: 7130 loss: 0.9079293608665466, accy: 0.72\n",
            "iter: 7140 loss: 0.9724481105804443, accy: 0.64\n",
            "iter: 7150 loss: 0.811320960521698, accy: 0.74\n",
            "iter: 7160 loss: 1.180230975151062, accy: 0.56\n",
            "iter: 7170 loss: 0.9722328186035156, accy: 0.62\n",
            "iter: 7180 loss: 0.8108199834823608, accy: 0.7\n",
            "iter: 7190 loss: 0.9216240644454956, accy: 0.72\n",
            "iter: 7200 loss: 0.8476685285568237, accy: 0.76\n",
            "iter: 7210 loss: 0.787386417388916, accy: 0.68\n",
            "iter: 7220 loss: 0.7824109792709351, accy: 0.8\n",
            "iter: 7230 loss: 0.9884556531906128, accy: 0.68\n",
            "iter: 7240 loss: 1.0254998207092285, accy: 0.72\n",
            "iter: 7250 loss: 0.6232457160949707, accy: 0.86\n",
            "iter: 7260 loss: 0.7174869775772095, accy: 0.74\n",
            "iter: 7270 loss: 0.9153537154197693, accy: 0.64\n",
            "iter: 7280 loss: 1.4271519184112549, accy: 0.62\n",
            "iter: 7290 loss: 0.9434683322906494, accy: 0.7\n",
            "iter: 7300 loss: 0.8553058505058289, accy: 0.72\n",
            "iter: 7310 loss: 0.7721613049507141, accy: 0.78\n",
            "iter: 7320 loss: 0.9378166198730469, accy: 0.7\n",
            "iter: 7330 loss: 0.7570582032203674, accy: 0.72\n",
            "iter: 7340 loss: 1.0130962133407593, accy: 0.7\n",
            "iter: 7350 loss: 0.8749203681945801, accy: 0.7\n",
            "iter: 7360 loss: 0.9904959201812744, accy: 0.82\n",
            "iter: 7370 loss: 0.6838359236717224, accy: 0.8\n",
            "iter: 7380 loss: 0.8032907843589783, accy: 0.76\n",
            "iter: 7390 loss: 0.9941006302833557, accy: 0.62\n",
            "iter: 7400 loss: 0.8674302697181702, accy: 0.76\n",
            "iter: 7410 loss: 0.6437102556228638, accy: 0.78\n",
            "iter: 7420 loss: 0.7367689609527588, accy: 0.76\n",
            "iter: 7430 loss: 0.6390730142593384, accy: 0.8\n",
            "iter: 7440 loss: 0.8694107532501221, accy: 0.74\n",
            "iter: 7450 loss: 0.7203037142753601, accy: 0.76\n",
            "iter: 7460 loss: 0.5983797311782837, accy: 0.82\n",
            "iter: 7470 loss: 1.2608976364135742, accy: 0.6\n",
            "iter: 7480 loss: 1.1302777528762817, accy: 0.6\n",
            "iter: 7490 loss: 0.7338559627532959, accy: 0.76\n",
            "iter: 7500 loss: 0.9480975866317749, accy: 0.6\n",
            "iter: 7510 loss: 1.2659344673156738, accy: 0.56\n",
            "iter: 7520 loss: 0.6224912405014038, accy: 0.82\n",
            "iter: 7530 loss: 0.7686269879341125, accy: 0.76\n",
            "iter: 7540 loss: 1.0462956428527832, accy: 0.64\n",
            "iter: 7550 loss: 0.8755797743797302, accy: 0.66\n",
            "iter: 7560 loss: 0.8812561631202698, accy: 0.74\n",
            "iter: 7570 loss: 0.9380220770835876, accy: 0.64\n",
            "iter: 7580 loss: 0.7877725958824158, accy: 0.82\n",
            "iter: 7590 loss: 0.6418737769126892, accy: 0.84\n",
            "iter: 7600 loss: 0.872416615486145, accy: 0.68\n",
            "iter: 7610 loss: 1.00498628616333, accy: 0.7\n",
            "iter: 7620 loss: 0.6774553656578064, accy: 0.74\n",
            "iter: 7630 loss: 1.0529894828796387, accy: 0.64\n",
            "iter: 7640 loss: 0.7385919690132141, accy: 0.72\n",
            "iter: 7650 loss: 0.7020300030708313, accy: 0.76\n",
            "iter: 7660 loss: 0.7877358794212341, accy: 0.78\n",
            "iter: 7670 loss: 0.9857451915740967, accy: 0.66\n",
            "iter: 7680 loss: 0.8231973052024841, accy: 0.68\n",
            "iter: 7690 loss: 0.6291075348854065, accy: 0.78\n",
            "iter: 7700 loss: 0.9104911088943481, accy: 0.64\n",
            "iter: 7710 loss: 0.8848952651023865, accy: 0.76\n",
            "iter: 7720 loss: 0.5234824419021606, accy: 0.78\n",
            "iter: 7730 loss: 0.79486083984375, accy: 0.68\n",
            "iter: 7740 loss: 0.7726998925209045, accy: 0.78\n",
            "iter: 7750 loss: 0.9006431698799133, accy: 0.74\n",
            "iter: 7760 loss: 0.6504075527191162, accy: 0.72\n",
            "iter: 7770 loss: 0.6052044034004211, accy: 0.78\n",
            "iter: 7780 loss: 0.8797827363014221, accy: 0.72\n",
            "iter: 7790 loss: 0.6810675859451294, accy: 0.72\n",
            "iter: 7800 loss: 1.0816411972045898, accy: 0.66\n",
            "iter: 7810 loss: 0.8715970516204834, accy: 0.72\n",
            "iter: 7820 loss: 0.6989372372627258, accy: 0.74\n",
            "iter: 7830 loss: 0.7582981586456299, accy: 0.8\n",
            "iter: 7840 loss: 1.08381187915802, accy: 0.58\n",
            "iter: 7850 loss: 0.9919118285179138, accy: 0.68\n",
            "iter: 7860 loss: 0.6719653606414795, accy: 0.8\n",
            "iter: 7870 loss: 0.576700747013092, accy: 0.78\n",
            "iter: 7880 loss: 0.9671496748924255, accy: 0.74\n",
            "iter: 7890 loss: 0.8712161183357239, accy: 0.7\n",
            "iter: 7900 loss: 0.9194641709327698, accy: 0.74\n",
            "iter: 7910 loss: 0.7952191829681396, accy: 0.78\n",
            "iter: 7920 loss: 0.7887609004974365, accy: 0.72\n",
            "iter: 7930 loss: 0.697469174861908, accy: 0.78\n",
            "iter: 7940 loss: 0.5918064713478088, accy: 0.88\n",
            "iter: 7950 loss: 0.9754012227058411, accy: 0.74\n",
            "iter: 7960 loss: 0.8806078433990479, accy: 0.74\n",
            "iter: 7970 loss: 0.8673629760742188, accy: 0.62\n",
            "iter: 7980 loss: 0.7979212403297424, accy: 0.7\n",
            "iter: 7990 loss: 0.7137182354927063, accy: 0.82\n",
            "iter: 8000 loss: 0.7885913848876953, accy: 0.82\n",
            "iter: 8010 loss: 1.0120168924331665, accy: 0.62\n",
            "iter: 8020 loss: 0.920259416103363, accy: 0.72\n",
            "iter: 8030 loss: 0.6226837038993835, accy: 0.84\n",
            "iter: 8040 loss: 0.5318331122398376, accy: 0.84\n",
            "iter: 8050 loss: 0.6113452911376953, accy: 0.8\n",
            "iter: 8060 loss: 1.0802147388458252, accy: 0.56\n",
            "iter: 8070 loss: 0.6260470747947693, accy: 0.82\n",
            "iter: 8080 loss: 0.8851472735404968, accy: 0.68\n",
            "iter: 8090 loss: 0.7159540057182312, accy: 0.76\n",
            "iter: 8100 loss: 0.6470957398414612, accy: 0.8\n",
            "iter: 8110 loss: 0.8189384937286377, accy: 0.7\n",
            "iter: 8120 loss: 0.73847496509552, accy: 0.8\n",
            "iter: 8130 loss: 0.7339057326316833, accy: 0.7\n",
            "iter: 8140 loss: 0.8494357466697693, accy: 0.74\n",
            "iter: 8150 loss: 0.7781123518943787, accy: 0.72\n",
            "iter: 8160 loss: 0.818846583366394, accy: 0.7\n",
            "iter: 8170 loss: 0.634962260723114, accy: 0.72\n",
            "iter: 8180 loss: 0.7072299718856812, accy: 0.78\n",
            "iter: 8190 loss: 0.8965723514556885, accy: 0.76\n",
            "iter: 8200 loss: 0.6879333257675171, accy: 0.82\n",
            "iter: 8210 loss: 0.8021522760391235, accy: 0.78\n",
            "iter: 8220 loss: 0.8797362446784973, accy: 0.78\n",
            "iter: 8230 loss: 1.1049445867538452, accy: 0.72\n",
            "iter: 8240 loss: 0.742979109287262, accy: 0.78\n",
            "iter: 8250 loss: 0.7985793948173523, accy: 0.7\n",
            "iter: 8260 loss: 0.7326063513755798, accy: 0.76\n",
            "iter: 8270 loss: 0.6390517950057983, accy: 0.78\n",
            "iter: 8280 loss: 0.7372421026229858, accy: 0.76\n",
            "iter: 8290 loss: 0.5690200328826904, accy: 0.82\n",
            "iter: 8300 loss: 0.7329022288322449, accy: 0.68\n",
            "iter: 8310 loss: 0.940570592880249, accy: 0.74\n",
            "iter: 8320 loss: 0.8457423448562622, accy: 0.7\n",
            "iter: 8330 loss: 0.4529743194580078, accy: 0.86\n",
            "iter: 8340 loss: 0.47665515542030334, accy: 0.84\n",
            "iter: 8350 loss: 0.8137499094009399, accy: 0.8\n",
            "iter: 8360 loss: 0.7293742299079895, accy: 0.68\n",
            "iter: 8370 loss: 0.731797993183136, accy: 0.72\n",
            "iter: 8380 loss: 0.703881025314331, accy: 0.78\n",
            "iter: 8390 loss: 0.8040046095848083, accy: 0.74\n",
            "iter: 8400 loss: 0.6553587913513184, accy: 0.78\n",
            "iter: 8410 loss: 0.8833510875701904, accy: 0.66\n",
            "iter: 8420 loss: 0.38971129059791565, accy: 0.9\n",
            "iter: 8430 loss: 0.6552231311798096, accy: 0.7\n",
            "iter: 8440 loss: 0.7558888792991638, accy: 0.82\n",
            "iter: 8450 loss: 0.745595395565033, accy: 0.78\n",
            "iter: 8460 loss: 0.5526495575904846, accy: 0.82\n",
            "iter: 8470 loss: 0.6870722770690918, accy: 0.82\n",
            "iter: 8480 loss: 0.572868824005127, accy: 0.8\n",
            "iter: 8490 loss: 0.7445392608642578, accy: 0.82\n",
            "iter: 8500 loss: 0.5904083251953125, accy: 0.86\n",
            "iter: 8510 loss: 0.7358232140541077, accy: 0.74\n",
            "iter: 8520 loss: 1.2530272006988525, accy: 0.58\n",
            "iter: 8530 loss: 0.8523761034011841, accy: 0.7\n",
            "iter: 8540 loss: 0.9603349566459656, accy: 0.66\n",
            "iter: 8550 loss: 0.7812841534614563, accy: 0.7\n",
            "iter: 8560 loss: 0.9080678820610046, accy: 0.7\n",
            "iter: 8570 loss: 0.8092877268791199, accy: 0.78\n",
            "iter: 8580 loss: 0.8755917549133301, accy: 0.7\n",
            "iter: 8590 loss: 0.4910869598388672, accy: 0.82\n",
            "iter: 8600 loss: 0.7104372382164001, accy: 0.76\n",
            "iter: 8610 loss: 0.9523244500160217, accy: 0.6\n",
            "iter: 8620 loss: 0.9737404584884644, accy: 0.74\n",
            "iter: 8630 loss: 0.5077497363090515, accy: 0.76\n",
            "iter: 8640 loss: 0.8837367296218872, accy: 0.7\n",
            "iter: 8650 loss: 0.5999869704246521, accy: 0.84\n",
            "iter: 8660 loss: 0.7542757391929626, accy: 0.78\n",
            "iter: 8670 loss: 0.8645321726799011, accy: 0.76\n",
            "iter: 8680 loss: 0.8308514952659607, accy: 0.8\n",
            "iter: 8690 loss: 0.7222354412078857, accy: 0.76\n",
            "iter: 8700 loss: 0.7380567789077759, accy: 0.74\n",
            "iter: 8710 loss: 0.9794235825538635, accy: 0.72\n",
            "iter: 8720 loss: 0.5187578797340393, accy: 0.8\n",
            "iter: 8730 loss: 0.8438664078712463, accy: 0.7\n",
            "iter: 8740 loss: 0.7894167900085449, accy: 0.72\n",
            "iter: 8750 loss: 0.8868647217750549, accy: 0.74\n",
            "iter: 8760 loss: 0.7749314904212952, accy: 0.64\n",
            "iter: 8770 loss: 0.6745224595069885, accy: 0.76\n",
            "iter: 8780 loss: 0.9815260171890259, accy: 0.66\n",
            "iter: 8790 loss: 0.8070068359375, accy: 0.72\n",
            "iter: 8800 loss: 0.8792281150817871, accy: 0.72\n",
            "iter: 8810 loss: 0.537554144859314, accy: 0.88\n",
            "iter: 8820 loss: 0.6224201321601868, accy: 0.78\n",
            "iter: 8830 loss: 0.7078990340232849, accy: 0.8\n",
            "iter: 8840 loss: 0.5975273847579956, accy: 0.76\n",
            "iter: 8850 loss: 0.7265733480453491, accy: 0.74\n",
            "iter: 8860 loss: 0.6000105142593384, accy: 0.78\n",
            "iter: 8870 loss: 0.852662205696106, accy: 0.68\n",
            "iter: 8880 loss: 0.6885654926300049, accy: 0.76\n",
            "iter: 8890 loss: 0.6795540452003479, accy: 0.74\n",
            "iter: 8900 loss: 0.5407516956329346, accy: 0.8\n",
            "iter: 8910 loss: 0.6925504803657532, accy: 0.74\n",
            "iter: 8920 loss: 0.5702577233314514, accy: 0.76\n",
            "iter: 8930 loss: 0.6807513236999512, accy: 0.8\n",
            "iter: 8940 loss: 0.7681095600128174, accy: 0.7\n",
            "iter: 8950 loss: 0.7845506072044373, accy: 0.76\n",
            "iter: 8960 loss: 0.9303851127624512, accy: 0.78\n",
            "iter: 8970 loss: 0.9039799571037292, accy: 0.76\n",
            "iter: 8980 loss: 0.8262925744056702, accy: 0.76\n",
            "iter: 8990 loss: 0.7556240558624268, accy: 0.8\n",
            "iter: 9000 loss: 1.1402994394302368, accy: 0.6\n",
            "iter: 9010 loss: 0.7613469958305359, accy: 0.8\n",
            "iter: 9020 loss: 0.7068448066711426, accy: 0.72\n",
            "iter: 9030 loss: 0.4679904282093048, accy: 0.82\n",
            "iter: 9040 loss: 0.915204644203186, accy: 0.66\n",
            "iter: 9050 loss: 0.7443703413009644, accy: 0.76\n",
            "iter: 9060 loss: 0.609127938747406, accy: 0.86\n",
            "iter: 9070 loss: 0.9104053974151611, accy: 0.72\n",
            "iter: 9080 loss: 0.44099387526512146, accy: 0.86\n",
            "iter: 9090 loss: 0.8147018551826477, accy: 0.7\n",
            "iter: 9100 loss: 0.6493737101554871, accy: 0.86\n",
            "iter: 9110 loss: 0.5709866285324097, accy: 0.78\n",
            "iter: 9120 loss: 0.644279420375824, accy: 0.78\n",
            "iter: 9130 loss: 0.7571117281913757, accy: 0.8\n",
            "iter: 9140 loss: 0.6605105400085449, accy: 0.74\n",
            "iter: 9150 loss: 0.5983015298843384, accy: 0.76\n",
            "iter: 9160 loss: 0.6330760717391968, accy: 0.82\n",
            "iter: 9170 loss: 0.6384300589561462, accy: 0.84\n",
            "iter: 9180 loss: 0.7749488949775696, accy: 0.72\n",
            "iter: 9190 loss: 0.801932692527771, accy: 0.72\n",
            "iter: 9200 loss: 0.7990763783454895, accy: 0.74\n",
            "iter: 9210 loss: 0.7574025988578796, accy: 0.74\n",
            "iter: 9220 loss: 0.8175182938575745, accy: 0.78\n",
            "iter: 9230 loss: 0.6308293342590332, accy: 0.72\n",
            "iter: 9240 loss: 0.9113311767578125, accy: 0.74\n",
            "iter: 9250 loss: 0.502110481262207, accy: 0.8\n",
            "iter: 9260 loss: 0.4297305643558502, accy: 0.86\n",
            "iter: 9270 loss: 0.8437651991844177, accy: 0.68\n",
            "iter: 9280 loss: 0.9866273403167725, accy: 0.64\n",
            "iter: 9290 loss: 0.7839570641517639, accy: 0.74\n",
            "iter: 9300 loss: 0.9016456604003906, accy: 0.68\n",
            "iter: 9310 loss: 0.7202682495117188, accy: 0.74\n",
            "iter: 9320 loss: 0.6227962374687195, accy: 0.76\n",
            "iter: 9330 loss: 0.6258196830749512, accy: 0.76\n",
            "iter: 9340 loss: 0.6921644806861877, accy: 0.74\n",
            "iter: 9350 loss: 0.6350494027137756, accy: 0.84\n",
            "iter: 9360 loss: 0.6500217318534851, accy: 0.78\n",
            "iter: 9370 loss: 0.3714843988418579, accy: 0.92\n",
            "iter: 9380 loss: 1.0715539455413818, accy: 0.66\n",
            "iter: 9390 loss: 0.6872749924659729, accy: 0.8\n",
            "iter: 9400 loss: 0.5869118571281433, accy: 0.86\n",
            "iter: 9410 loss: 0.613243579864502, accy: 0.74\n",
            "iter: 9420 loss: 0.6567981839179993, accy: 0.82\n",
            "iter: 9430 loss: 0.6763327717781067, accy: 0.8\n",
            "iter: 9440 loss: 0.7467994689941406, accy: 0.74\n",
            "iter: 9450 loss: 0.7191752791404724, accy: 0.78\n",
            "iter: 9460 loss: 0.8392562866210938, accy: 0.72\n",
            "iter: 9470 loss: 0.8910654187202454, accy: 0.68\n",
            "iter: 9480 loss: 0.6088975667953491, accy: 0.8\n",
            "iter: 9490 loss: 0.7381558418273926, accy: 0.66\n",
            "iter: 9500 loss: 0.6222676038742065, accy: 0.74\n",
            "iter: 9510 loss: 0.759615421295166, accy: 0.76\n",
            "iter: 9520 loss: 0.57053542137146, accy: 0.78\n",
            "iter: 9530 loss: 0.9063818454742432, accy: 0.68\n",
            "iter: 9540 loss: 0.8183212280273438, accy: 0.68\n",
            "iter: 9550 loss: 1.1808451414108276, accy: 0.68\n",
            "iter: 9560 loss: 1.0096489191055298, accy: 0.68\n",
            "iter: 9570 loss: 0.9726662635803223, accy: 0.66\n",
            "iter: 9580 loss: 0.663530707359314, accy: 0.74\n",
            "iter: 9590 loss: 0.8066582679748535, accy: 0.8\n",
            "iter: 9600 loss: 0.7541711926460266, accy: 0.7\n",
            "iter: 9610 loss: 1.0718774795532227, accy: 0.66\n",
            "iter: 9620 loss: 0.6843457221984863, accy: 0.74\n",
            "iter: 9630 loss: 0.6696927547454834, accy: 0.8\n",
            "iter: 9640 loss: 0.4959082007408142, accy: 0.78\n",
            "iter: 9650 loss: 0.6307171583175659, accy: 0.72\n",
            "iter: 9660 loss: 0.6560258269309998, accy: 0.78\n",
            "iter: 9670 loss: 0.8903839588165283, accy: 0.68\n",
            "iter: 9680 loss: 0.5103252530097961, accy: 0.8\n",
            "iter: 9690 loss: 0.6593955159187317, accy: 0.76\n",
            "iter: 9700 loss: 0.7144321203231812, accy: 0.78\n",
            "iter: 9710 loss: 0.4927283525466919, accy: 0.82\n",
            "iter: 9720 loss: 0.6946818828582764, accy: 0.82\n",
            "iter: 9730 loss: 0.6743942499160767, accy: 0.76\n",
            "iter: 9740 loss: 0.43483737111091614, accy: 0.88\n",
            "iter: 9750 loss: 0.7075462341308594, accy: 0.82\n",
            "iter: 9760 loss: 0.6133034825325012, accy: 0.82\n",
            "iter: 9770 loss: 0.8229817152023315, accy: 0.72\n",
            "iter: 9780 loss: 0.5985348224639893, accy: 0.74\n",
            "iter: 9790 loss: 0.8053348064422607, accy: 0.8\n",
            "iter: 9800 loss: 0.6431585550308228, accy: 0.8\n",
            "iter: 9810 loss: 0.40819981694221497, accy: 0.84\n",
            "iter: 9820 loss: 0.5106040835380554, accy: 0.8\n",
            "iter: 9830 loss: 1.065772533416748, accy: 0.76\n",
            "iter: 9840 loss: 0.7646709680557251, accy: 0.82\n",
            "iter: 9850 loss: 0.7919124364852905, accy: 0.72\n",
            "iter: 9860 loss: 0.3933284878730774, accy: 0.88\n",
            "iter: 9870 loss: 0.63922518491745, accy: 0.76\n",
            "iter: 9880 loss: 0.7826359272003174, accy: 0.76\n",
            "iter: 9890 loss: 0.7625500559806824, accy: 0.8\n",
            "iter: 9900 loss: 0.950588047504425, accy: 0.76\n",
            "iter: 9910 loss: 0.6681370735168457, accy: 0.74\n",
            "iter: 9920 loss: 0.8436413407325745, accy: 0.8\n",
            "iter: 9930 loss: 0.6058156490325928, accy: 0.86\n",
            "iter: 9940 loss: 0.9128202199935913, accy: 0.7\n",
            "iter: 9950 loss: 0.5323750972747803, accy: 0.84\n",
            "iter: 9960 loss: 0.6035068035125732, accy: 0.76\n",
            "iter: 9970 loss: 0.8587550520896912, accy: 0.7\n",
            "iter: 9980 loss: 0.6619951725006104, accy: 0.78\n",
            "iter: 9990 loss: 0.4124206602573395, accy: 0.9\n",
            "iter: 10000 loss: 0.6735485792160034, accy: 0.8\n",
            "iter: 10010 loss: 0.6468968391418457, accy: 0.8\n",
            "iter: 10020 loss: 0.5981003046035767, accy: 0.82\n",
            "iter: 10030 loss: 1.0276556015014648, accy: 0.68\n",
            "iter: 10040 loss: 0.5807058811187744, accy: 0.8\n",
            "iter: 10050 loss: 1.0114388465881348, accy: 0.8\n",
            "iter: 10060 loss: 0.4703860878944397, accy: 0.78\n",
            "iter: 10070 loss: 0.5421227812767029, accy: 0.82\n",
            "iter: 10080 loss: 0.8461265563964844, accy: 0.7\n",
            "iter: 10090 loss: 1.1145955324172974, accy: 0.7\n",
            "iter: 10100 loss: 0.600620687007904, accy: 0.8\n",
            "iter: 10110 loss: 1.2011210918426514, accy: 0.66\n",
            "iter: 10120 loss: 0.8415696620941162, accy: 0.68\n",
            "iter: 10130 loss: 0.7799074053764343, accy: 0.74\n",
            "iter: 10140 loss: 0.6777047514915466, accy: 0.7\n",
            "iter: 10150 loss: 0.7578631639480591, accy: 0.76\n",
            "iter: 10160 loss: 0.9271669983863831, accy: 0.7\n",
            "iter: 10170 loss: 0.6284389495849609, accy: 0.78\n",
            "iter: 10180 loss: 0.6312549114227295, accy: 0.78\n",
            "iter: 10190 loss: 0.6563102006912231, accy: 0.78\n",
            "iter: 10200 loss: 0.8072776794433594, accy: 0.74\n",
            "iter: 10210 loss: 0.7493604421615601, accy: 0.82\n",
            "iter: 10220 loss: 0.730632483959198, accy: 0.7\n",
            "iter: 10230 loss: 0.702595591545105, accy: 0.74\n",
            "iter: 10240 loss: 0.6515336036682129, accy: 0.74\n",
            "iter: 10250 loss: 0.404626727104187, accy: 0.86\n",
            "iter: 10260 loss: 0.5383414626121521, accy: 0.86\n",
            "iter: 10270 loss: 0.5308263301849365, accy: 0.78\n",
            "iter: 10280 loss: 0.5696962475776672, accy: 0.78\n",
            "iter: 10290 loss: 0.8482694029808044, accy: 0.82\n",
            "iter: 10300 loss: 0.5927439332008362, accy: 0.76\n",
            "iter: 10310 loss: 0.658562183380127, accy: 0.78\n",
            "iter: 10320 loss: 0.7270534038543701, accy: 0.82\n",
            "iter: 10330 loss: 0.6958193778991699, accy: 0.8\n",
            "iter: 10340 loss: 0.6596399545669556, accy: 0.72\n",
            "iter: 10350 loss: 0.4465511441230774, accy: 0.84\n",
            "iter: 10360 loss: 0.6476995944976807, accy: 0.82\n",
            "iter: 10370 loss: 0.6391478180885315, accy: 0.78\n",
            "iter: 10380 loss: 0.7883734107017517, accy: 0.78\n",
            "iter: 10390 loss: 0.4543558359146118, accy: 0.86\n",
            "iter: 10400 loss: 0.6010003685951233, accy: 0.86\n",
            "iter: 10410 loss: 0.8513720631599426, accy: 0.72\n",
            "iter: 10420 loss: 0.679171621799469, accy: 0.7\n",
            "iter: 10430 loss: 0.5434466004371643, accy: 0.8\n",
            "iter: 10440 loss: 0.9100329875946045, accy: 0.7\n",
            "iter: 10450 loss: 0.40983378887176514, accy: 0.86\n",
            "iter: 10460 loss: 0.5810213685035706, accy: 0.8\n",
            "iter: 10470 loss: 0.7878291606903076, accy: 0.78\n",
            "iter: 10480 loss: 0.7038179039955139, accy: 0.72\n",
            "iter: 10490 loss: 0.8567073345184326, accy: 0.78\n",
            "iter: 10500 loss: 0.5235241055488586, accy: 0.78\n",
            "iter: 10510 loss: 0.6812475323677063, accy: 0.76\n",
            "iter: 10520 loss: 0.4882834255695343, accy: 0.82\n",
            "iter: 10530 loss: 0.6826199889183044, accy: 0.74\n",
            "iter: 10540 loss: 0.8117244243621826, accy: 0.7\n",
            "iter: 10550 loss: 0.6456411480903625, accy: 0.78\n",
            "iter: 10560 loss: 0.722881019115448, accy: 0.76\n",
            "iter: 10570 loss: 0.6195762753486633, accy: 0.86\n",
            "iter: 10580 loss: 0.6179417967796326, accy: 0.8\n",
            "iter: 10590 loss: 0.6888101100921631, accy: 0.78\n",
            "iter: 10600 loss: 0.6781020164489746, accy: 0.78\n",
            "iter: 10610 loss: 0.5162190198898315, accy: 0.88\n",
            "iter: 10620 loss: 0.6472256183624268, accy: 0.8\n",
            "iter: 10630 loss: 0.5891605615615845, accy: 0.76\n",
            "iter: 10640 loss: 0.6249074339866638, accy: 0.78\n",
            "iter: 10650 loss: 0.74627685546875, accy: 0.74\n",
            "iter: 10660 loss: 0.6343387961387634, accy: 0.82\n",
            "iter: 10670 loss: 0.543371856212616, accy: 0.82\n",
            "iter: 10680 loss: 0.4833788275718689, accy: 0.8\n",
            "iter: 10690 loss: 0.7658588886260986, accy: 0.72\n",
            "iter: 10700 loss: 1.0358105897903442, accy: 0.74\n",
            "iter: 10710 loss: 0.7008920907974243, accy: 0.8\n",
            "iter: 10720 loss: 1.120800495147705, accy: 0.68\n",
            "iter: 10730 loss: 0.6837286949157715, accy: 0.7\n",
            "iter: 10740 loss: 0.8196800351142883, accy: 0.68\n",
            "iter: 10750 loss: 0.8878633379936218, accy: 0.68\n",
            "iter: 10760 loss: 0.9288869500160217, accy: 0.76\n",
            "iter: 10770 loss: 0.5164785981178284, accy: 0.84\n",
            "iter: 10780 loss: 0.782358705997467, accy: 0.72\n",
            "iter: 10790 loss: 0.49547356367111206, accy: 0.84\n",
            "iter: 10800 loss: 0.5506885647773743, accy: 0.82\n",
            "iter: 10810 loss: 0.385741651058197, accy: 0.86\n",
            "iter: 10820 loss: 0.6788163781166077, accy: 0.82\n",
            "iter: 10830 loss: 0.6492899060249329, accy: 0.8\n",
            "iter: 10840 loss: 0.6216373443603516, accy: 0.82\n",
            "iter: 10850 loss: 0.852947473526001, accy: 0.74\n",
            "iter: 10860 loss: 0.6706193685531616, accy: 0.78\n",
            "iter: 10870 loss: 0.5204579830169678, accy: 0.8\n",
            "iter: 10880 loss: 0.6947739124298096, accy: 0.72\n",
            "iter: 10890 loss: 0.4473751485347748, accy: 0.84\n",
            "iter: 10900 loss: 0.6005207300186157, accy: 0.78\n",
            "iter: 10910 loss: 0.5147219896316528, accy: 0.82\n",
            "iter: 10920 loss: 0.6907151937484741, accy: 0.78\n",
            "iter: 10930 loss: 0.5755702257156372, accy: 0.8\n",
            "iter: 10940 loss: 0.9479444622993469, accy: 0.74\n",
            "iter: 10950 loss: 0.6278972029685974, accy: 0.76\n",
            "iter: 10960 loss: 0.5351681113243103, accy: 0.8\n",
            "iter: 10970 loss: 0.6211889386177063, accy: 0.76\n",
            "iter: 10980 loss: 0.7170864343643188, accy: 0.78\n",
            "iter: 10990 loss: 0.85272216796875, accy: 0.7\n",
            "iter: 11000 loss: 0.508481502532959, accy: 0.8\n",
            "iter: 11010 loss: 0.9009811878204346, accy: 0.72\n",
            "iter: 11020 loss: 0.6084038615226746, accy: 0.86\n",
            "iter: 11030 loss: 0.5479729175567627, accy: 0.82\n",
            "iter: 11040 loss: 0.5633759498596191, accy: 0.82\n",
            "iter: 11050 loss: 0.4497164785861969, accy: 0.88\n",
            "iter: 11060 loss: 0.4334806799888611, accy: 0.84\n",
            "iter: 11070 loss: 0.5485678315162659, accy: 0.8\n",
            "iter: 11080 loss: 0.5426267981529236, accy: 0.84\n",
            "iter: 11090 loss: 0.7575113773345947, accy: 0.76\n",
            "iter: 11100 loss: 0.9162726402282715, accy: 0.78\n",
            "iter: 11110 loss: 0.8206601738929749, accy: 0.76\n",
            "iter: 11120 loss: 0.9665066599845886, accy: 0.66\n",
            "iter: 11130 loss: 0.542178750038147, accy: 0.88\n",
            "iter: 11140 loss: 0.5227521657943726, accy: 0.78\n",
            "iter: 11150 loss: 0.7086561322212219, accy: 0.82\n",
            "iter: 11160 loss: 0.5819458961486816, accy: 0.78\n",
            "iter: 11170 loss: 0.43956807255744934, accy: 0.88\n",
            "iter: 11180 loss: 0.39815035462379456, accy: 0.9\n",
            "iter: 11190 loss: 0.6867157816886902, accy: 0.78\n",
            "iter: 11200 loss: 0.7650432586669922, accy: 0.72\n",
            "iter: 11210 loss: 0.5741286873817444, accy: 0.82\n",
            "iter: 11220 loss: 0.49051380157470703, accy: 0.84\n",
            "iter: 11230 loss: 0.6792583465576172, accy: 0.78\n",
            "iter: 11240 loss: 0.5498353242874146, accy: 0.82\n",
            "iter: 11250 loss: 0.4050406515598297, accy: 0.88\n",
            "iter: 11260 loss: 0.6945202350616455, accy: 0.7\n",
            "iter: 11270 loss: 0.7186022400856018, accy: 0.72\n",
            "iter: 11280 loss: 0.7246353626251221, accy: 0.78\n",
            "iter: 11290 loss: 0.5301571488380432, accy: 0.8\n",
            "iter: 11300 loss: 0.6115809082984924, accy: 0.8\n",
            "iter: 11310 loss: 0.7582086324691772, accy: 0.8\n",
            "iter: 11320 loss: 0.5528903007507324, accy: 0.86\n",
            "iter: 11330 loss: 0.5922767519950867, accy: 0.86\n",
            "iter: 11340 loss: 0.7957254648208618, accy: 0.76\n",
            "iter: 11350 loss: 0.6323502063751221, accy: 0.78\n",
            "iter: 11360 loss: 0.7354427576065063, accy: 0.7\n",
            "iter: 11370 loss: 0.45715785026550293, accy: 0.86\n",
            "iter: 11380 loss: 0.517036497592926, accy: 0.82\n",
            "iter: 11390 loss: 0.6843198537826538, accy: 0.68\n",
            "iter: 11400 loss: 0.7159485816955566, accy: 0.74\n",
            "iter: 11410 loss: 0.7967981100082397, accy: 0.7\n",
            "iter: 11420 loss: 0.5409704446792603, accy: 0.88\n",
            "iter: 11430 loss: 0.6475735306739807, accy: 0.76\n",
            "iter: 11440 loss: 0.6722516417503357, accy: 0.84\n",
            "iter: 11450 loss: 0.5589489936828613, accy: 0.84\n",
            "iter: 11460 loss: 0.8180985450744629, accy: 0.7\n",
            "iter: 11470 loss: 0.5805806517601013, accy: 0.86\n",
            "iter: 11480 loss: 0.5899577140808105, accy: 0.78\n",
            "iter: 11490 loss: 0.5305435061454773, accy: 0.84\n",
            "iter: 11500 loss: 0.7260249257087708, accy: 0.68\n",
            "iter: 11510 loss: 0.5958066582679749, accy: 0.76\n",
            "iter: 11520 loss: 0.7200026512145996, accy: 0.76\n",
            "iter: 11530 loss: 0.7418924570083618, accy: 0.78\n",
            "iter: 11540 loss: 0.6262139081954956, accy: 0.84\n",
            "iter: 11550 loss: 0.7011162042617798, accy: 0.78\n",
            "iter: 11560 loss: 1.0458213090896606, accy: 0.66\n",
            "iter: 11570 loss: 0.8114653825759888, accy: 0.76\n",
            "iter: 11580 loss: 0.6211778521537781, accy: 0.76\n",
            "iter: 11590 loss: 0.7109590172767639, accy: 0.66\n",
            "iter: 11600 loss: 0.9308438897132874, accy: 0.74\n",
            "iter: 11610 loss: 0.8997524976730347, accy: 0.7\n",
            "iter: 11620 loss: 0.6813682317733765, accy: 0.7\n",
            "iter: 11630 loss: 0.6928093433380127, accy: 0.74\n",
            "iter: 11640 loss: 0.7108733654022217, accy: 0.82\n",
            "iter: 11650 loss: 0.4242994785308838, accy: 0.84\n",
            "iter: 11660 loss: 0.49607712030410767, accy: 0.86\n",
            "iter: 11670 loss: 0.7174341678619385, accy: 0.74\n",
            "iter: 11680 loss: 0.7993477582931519, accy: 0.76\n",
            "iter: 11690 loss: 0.47166943550109863, accy: 0.8\n",
            "iter: 11700 loss: 0.6655054688453674, accy: 0.72\n",
            "iter: 11710 loss: 0.4981875717639923, accy: 0.84\n",
            "iter: 11720 loss: 0.6346579194068909, accy: 0.78\n",
            "iter: 11730 loss: 0.5755271315574646, accy: 0.8\n",
            "iter: 11740 loss: 0.7045554518699646, accy: 0.76\n",
            "iter: 11750 loss: 0.4431021809577942, accy: 0.9\n",
            "iter: 11760 loss: 0.4993685185909271, accy: 0.88\n",
            "iter: 11770 loss: 0.6360927224159241, accy: 0.8\n",
            "iter: 11780 loss: 0.5595537424087524, accy: 0.82\n",
            "iter: 11790 loss: 0.34890374541282654, accy: 0.92\n",
            "iter: 11800 loss: 0.762991726398468, accy: 0.84\n",
            "iter: 11810 loss: 0.6070181131362915, accy: 0.78\n",
            "iter: 11820 loss: 0.7543795108795166, accy: 0.76\n",
            "iter: 11830 loss: 1.0498595237731934, accy: 0.66\n",
            "iter: 11840 loss: 0.5923505425453186, accy: 0.76\n",
            "iter: 11850 loss: 0.8819752335548401, accy: 0.72\n",
            "iter: 11860 loss: 0.5380427837371826, accy: 0.8\n",
            "iter: 11870 loss: 0.8776034712791443, accy: 0.68\n",
            "iter: 11880 loss: 0.9051854014396667, accy: 0.7\n",
            "iter: 11890 loss: 0.7576666474342346, accy: 0.76\n",
            "iter: 11900 loss: 0.6910605430603027, accy: 0.76\n",
            "iter: 11910 loss: 0.7498655915260315, accy: 0.8\n",
            "iter: 11920 loss: 0.8365886211395264, accy: 0.76\n",
            "iter: 11930 loss: 0.8785768747329712, accy: 0.7\n",
            "iter: 11940 loss: 0.6619002819061279, accy: 0.7\n",
            "iter: 11950 loss: 0.7245627641677856, accy: 0.78\n",
            "iter: 11960 loss: 0.5013495683670044, accy: 0.88\n",
            "iter: 11970 loss: 1.0819103717803955, accy: 0.78\n",
            "iter: 11980 loss: 0.5802275538444519, accy: 0.8\n",
            "iter: 11990 loss: 0.684630274772644, accy: 0.74\n",
            "iter: 12000 loss: 0.5106592178344727, accy: 0.86\n",
            "testing accy:  0.9087\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.065 MB of 0.065 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6458fb7a8cf0418690dfc81e02109e19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Testing Accuracy</td><td>▁</td></tr><tr><td>accy</td><td>▁▁▄▃▄▄▅▅▆▇▅▆▆▆▆▆▆▆▆▆▇▇▆▇█▆█▆▆▇▇▇█▆█▆█▆▇█</td></tr><tr><td>iter</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▇▅▆▅▄▄▃▄▄▄▃▃▄▃▃▃▃▃▂▃▃▃▂▂▃▃▃▂▂▁▃▁▄▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Testing Accuracy</td><td>0.9087</td></tr><tr><td>accy</td><td>0.86</td></tr><tr><td>iter</td><td>12000</td></tr><tr><td>loss</td><td>0.51066</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hw3</strong> at: <a href='https://wandb.ai/hw6_csci436/CSCI_436/runs/db15qdjh' target=\"_blank\">https://wandb.ai/hw6_csci436/CSCI_436/runs/db15qdjh</a><br/> View project at: <a href='https://wandb.ai/hw6_csci436/CSCI_436' target=\"_blank\">https://wandb.ai/hw6_csci436/CSCI_436</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240418_194859-db15qdjh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}